{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80040987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1d66e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "DATA_DIR = r'D:\\data'\n",
    "df = pd.read_csv(r'D:\\data\\dataset.csv')\n",
    "pd.set_option('display.max_columns',None)\n",
    "np.set_printoptions(threshold=np.inf) \n",
    "train = {}\n",
    "train = df.to_dict()\n",
    "train1 = train['Sequence']\n",
    "train2 = train['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3e5a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "char_to_num = {\n",
    "    'A': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7,\n",
    "    'I': 8, 'K': 9, 'L': 10, 'M': 11, 'N': 12, 'P': 13,\n",
    "    'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'V': 18, 'W': 19, 'Y': 20\n",
    "}\n",
    "\n",
    "def OE1(seq_temp1):\n",
    "    fea1 = [[char_to_num.get(char, 0)] for char in seq_temp1]\n",
    "    return fea1\n",
    "\n",
    "train1_oe1 = [OE1(train1[i]) for i in train1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1977df5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(306, 20)\n",
      "[[0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.5        0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.5        0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.5        0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.5        0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.5        0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.5       ]\n",
      " [0.         0.5        0.         0.         0.5        0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.5        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.5        0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.5        0.         0.         0.         0.         0.\n",
      "  0.5        0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.5        0.         0.\n",
      "  0.         0.         0.5        0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.5        0.         0.         0.         0.\n",
      "  0.5        0.        ]\n",
      " [0.         0.5        0.         0.         0.         0.\n",
      "  0.         0.         0.5        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.5        0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.5        0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.5        0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.5       ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.5        0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.5        0.         0.\n",
      "  0.5        0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.5        0.5        0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.5        0.         0.5        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.5        0.5       ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.5        0.5        0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.5        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.5        0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.5        0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.5        0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.5        0.\n",
      "  0.         0.        ]\n",
      " [0.         0.5        0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.5        0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.5        0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.5        0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.5        0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.5        0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.5        0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.5        0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.5       ]\n",
      " [0.5        0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.5        0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.5        0.         0.5\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.5        0.         0.         0.5        0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.5        0.         0.5        0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.5        0.         0.         0.5        0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.5       ]\n",
      " [0.         0.         0.5        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.5        0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.5        0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.33333333 0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333]\n",
      " [0.         0.         0.33333333 0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.33333333 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.33333333 0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33333333 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.66666667\n",
      "  0.         0.         0.33333333 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.66666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.33333333 0.         0.\n",
      "  0.         0.33333333]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.33333333]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333 0.         0.         0.         0.\n",
      "  0.         0.33333333 0.         0.         0.         0.\n",
      "  0.33333333 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.33333333 0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.33333333 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33333333 0.33333333 0.33333333 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333 0.         0.         0.         0.\n",
      "  0.         0.33333333]\n",
      " [0.         0.33333333 0.         0.         0.         0.33333333\n",
      "  0.33333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.33333333 0.         0.33333333 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333 0.         0.         0.         0.\n",
      "  0.         0.33333333]\n",
      " [0.         0.         0.33333333 0.         0.         0.33333333\n",
      "  0.33333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.        ]\n",
      " [0.         0.         0.         0.33333333 0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33333333 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.33333333 0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.33333333 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.33333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333]\n",
      " [0.         0.         0.         0.         0.33333333 0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333 0.         0.         0.         0.\n",
      "  0.         0.33333333]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33333333 0.         0.         0.33333333\n",
      "  0.         0.        ]\n",
      " [0.33333333 0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.33333333 0.         0.         0.33333333\n",
      "  0.         0.         0.         0.33333333 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.33333333 0.33333333\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.33333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33333333 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.33333333 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.         0.         0.33333333 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.33333333 0.         0.         0.         0.         0.33333333\n",
      "  0.         0.33333333 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.33333333 0.         0.33333333\n",
      "  0.33333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.33333333 0.         0.         0.         0.\n",
      "  0.         0.33333333 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.        ]\n",
      " [0.         0.         0.         0.         0.33333333 0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.33333333 0.         0.\n",
      "  0.33333333 0.         0.         0.         0.         0.33333333\n",
      "  0.         0.        ]\n",
      " [0.         0.33333333 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.33333333]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.         0.33333333 0.33333333 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.         0.33333333 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.33333333 0.33333333 0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.33333333 0.         0.\n",
      "  0.         0.         0.33333333 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.        ]\n",
      " [0.33333333 0.         0.         0.         0.33333333 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.        ]\n",
      " [0.         0.33333333 0.         0.         0.         0.\n",
      "  0.33333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.33333333 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.66666667]\n",
      " [0.         0.33333333 0.33333333 0.         0.         0.\n",
      "  0.         0.         0.         0.33333333 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.33333333 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.33333333 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333 0.         0.         0.         0.\n",
      "  0.         0.33333333 0.         0.         0.         0.33333333\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333 0.         0.         0.33333333 0.\n",
      "  0.         0.33333333 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33333333 0.         0.         0.\n",
      "  0.         0.33333333]\n",
      " [0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.         0.33333333 0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.33333333 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.33333333]\n",
      " [0.         0.         0.         0.33333333 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33333333 0.         0.         0.33333333\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.33333333 0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.25       0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.25       0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.25       0.         0.25\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.25       0.         0.         0.25       0.         0.\n",
      "  0.         0.        ]\n",
      " [0.25       0.         0.         0.         0.         0.25\n",
      "  0.         0.         0.25       0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.25      ]\n",
      " [0.         0.         0.         0.         0.25       0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.25      ]\n",
      " [0.         0.         0.         0.         0.5        0.25\n",
      "  0.         0.         0.25       0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.25       0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.25       0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.25       0.5\n",
      "  0.         0.         0.25       0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.25       0.         0.         0.         0.         0.25\n",
      "  0.         0.         0.25       0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.25      ]\n",
      " [0.         0.         0.         0.         0.         0.25\n",
      "  0.         0.         0.         0.         0.25       0.25\n",
      "  0.25       0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.25\n",
      "  0.         0.         0.25       0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.25      ]\n",
      " [0.         0.         0.25       0.         0.         0.25\n",
      "  0.         0.25       0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25       0.25\n",
      "  0.25       0.         0.         0.25       0.         0.\n",
      "  0.         0.        ]\n",
      " [0.5        0.25       0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.25      ]\n",
      " [0.         0.         0.25       0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.        ]\n",
      " [0.25       0.         0.         0.         0.         0.25\n",
      "  0.         0.         0.25       0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.25       0.25\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.25       0.         0.\n",
      "  0.         0.25      ]\n",
      " [0.5        0.         0.         0.         0.25       0.25\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.75       0.25       0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.25\n",
      "  0.         0.         0.         0.25       0.         0.\n",
      "  0.25       0.         0.         0.         0.25       0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.25       0.         0.25\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.25       0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.25       0.         0.25       0.         0.         0.\n",
      "  0.25       0.        ]\n",
      " [0.         0.         0.         0.         0.         0.25\n",
      "  0.         0.25       0.         0.         0.         0.\n",
      "  0.25       0.         0.         0.25       0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.25       0.25\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.25       0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.25       0.         0.         0.25\n",
      "  0.         0.         0.25       0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.25      ]\n",
      " [0.25       0.         0.         0.         0.25       0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.25       0.         0.         0.         0.\n",
      "  0.         0.25      ]\n",
      " [0.         0.         0.         0.         0.25       0.25\n",
      "  0.         0.25       0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.        ]\n",
      " [0.25       0.         0.5        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.25       0.         0.25       0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.25      ]\n",
      " [0.25       0.         0.75       0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.25       0.25\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.25       0.         0.         0.         0.25       0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.25       0.         0.25       0.         0.\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.        ]\n",
      " [0.         0.25       0.         0.         0.25       0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.25       0.         0.         0.         0.         0.25\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.25       0.         0.         0.\n",
      "  0.         0.25      ]\n",
      " [0.         0.         0.25       0.         0.         0.25\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.25       0.         0.         0.         0.25       0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.25       0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.25       0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.25\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.25       0.         0.         0.25       0.         0.\n",
      "  0.         0.        ]\n",
      " [0.25       0.25       0.         0.         0.         0.25\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.25       0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.2        0.         0.4        0.         0.         0.2\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.2        0.2\n",
      "  0.         0.         0.         0.         0.2        0.2\n",
      "  0.         0.         0.         0.2        0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.2        0.         0.2\n",
      "  0.         0.         0.         0.         0.2        0.2\n",
      "  0.         0.         0.         0.2        0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.2        0.         0.         0.2        0.4\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.2       ]\n",
      " [0.2        0.         0.         0.         0.2        0.2\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.2        0.         0.         0.\n",
      "  0.         0.2       ]\n",
      " [0.         0.         0.         0.         0.         0.2\n",
      "  0.         0.         0.8        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.2        0.2        0.2\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.2        0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.2        0.\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.2        0.         0.2        0.         0.         0.\n",
      "  0.2        0.        ]\n",
      " [0.         0.         0.         0.         0.         0.4\n",
      "  0.         0.2        0.         0.         0.         0.\n",
      "  0.2        0.         0.         0.2        0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.2        0.         0.         0.2\n",
      "  0.         0.         0.2        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.         0.2       ]\n",
      " [0.4        0.         0.         0.         0.2        0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.2        0.         0.         0.         0.\n",
      "  0.         0.2       ]\n",
      " [0.         0.         0.2        0.         0.         0.2\n",
      "  0.         0.         0.2        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.         0.2       ]\n",
      " [0.         0.         0.2        0.         0.         0.2\n",
      "  0.         0.2        0.         0.2        0.         0.\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.2\n",
      "  0.         0.         0.         0.         0.2        0.2\n",
      "  0.2        0.         0.         0.2        0.         0.\n",
      "  0.         0.        ]\n",
      " [0.4        0.2        0.         0.         0.2        0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.2       ]\n",
      " [0.2        0.         0.4        0.         0.         0.2\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.2\n",
      "  0.2        0.         0.2        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.         0.2       ]\n",
      " [0.2        0.         0.2        0.         0.         0.2\n",
      "  0.         0.         0.2        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.         0.        ]\n",
      " [0.2        0.         0.6        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.2        0.         0.2        0.2\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.2        0.         0.         0.         0.2        0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.2        0.2\n",
      "  0.         0.2        0.         0.2        0.         0.\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.         0.        ]\n",
      " [0.4        0.         0.         0.         0.2        0.2\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.2        0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.6        0.2        0.         0.         0.2        0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.2        0.2\n",
      "  0.         0.         0.         0.2        0.         0.\n",
      "  0.2        0.         0.         0.         0.2        0.\n",
      "  0.         0.        ]\n",
      " [0.2        0.         0.2        0.         0.         0.2\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2        0.2\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.2        0.2        0.2\n",
      "  0.         0.         0.4        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.2        0.         0.2\n",
      "  0.         0.         0.         0.         0.2        0.2\n",
      "  0.2        0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.2        0.2\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.2        0.         0.         0.2        0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.2        0.2\n",
      "  0.         0.2        0.         0.         0.         0.\n",
      "  0.2        0.         0.         0.         0.2        0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.2        0.2\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.2        0.         0.         0.2        0.         0.\n",
      "  0.         0.2       ]\n",
      " [0.         0.         0.2        0.         0.         0.4\n",
      "  0.         0.         0.2        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.2        0.         0.         0.2\n",
      "  0.         0.         0.         0.2        0.         0.\n",
      "  0.2        0.         0.         0.         0.2        0.\n",
      "  0.         0.        ]\n",
      " [0.2        0.2        0.         0.         0.         0.2\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.2        0.         0.         0.         0.\n",
      "  0.         0.2       ]\n",
      " [0.4        0.         0.         0.         0.         0.4\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.2        0.        ]\n",
      " [0.         0.         0.         0.         0.16666667 0.33333333\n",
      "  0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.16666667 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.16666667 0.         0.16666667 0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.16666667 0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.14285714 0.         0.28571429 0.28571429\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.14285714 0.\n",
      "  0.         0.14285714]\n",
      " [0.         0.         0.16666667 0.         0.5        0.16666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.16666667 0.         0.         0.16666667\n",
      "  0.         0.16666667 0.         0.16666667 0.         0.\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.        ]\n",
      " [0.33333333 0.         0.         0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.16666667 0.         0.         0.         0.\n",
      "  0.         0.16666667]\n",
      " [0.16666667 0.16666667 0.         0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.16666667]\n",
      " [0.         0.         0.16666667 0.         0.16666667 0.33333333\n",
      "  0.16666667 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.16666667 0.         0.5        0.         0.         0.16666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.16666667 0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.16666667 0.         0.16666667 0.\n",
      "  0.         0.16666667]\n",
      " [0.         0.         0.16666667 0.         0.         0.33333333\n",
      "  0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.16666667]\n",
      " [0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.16666667 0.         0.         0.16666667 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.16666667 0.33333333\n",
      "  0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.16666667 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.16666667 0.         0.         0.16666667 0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.16666667]\n",
      " [0.         0.         0.16666667 0.         0.         0.16666667\n",
      "  0.16666667 0.         0.16666667 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.16666667]\n",
      " [0.         0.         0.16666667 0.         0.16666667 0.33333333\n",
      "  0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.16666667 0.         0.16666667 0.         0.         0.33333333\n",
      "  0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.5        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.33333333 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.16666667 0.         0.16666667 0.16666667\n",
      "  0.         0.16666667 0.         0.16666667 0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.16666667\n",
      "  0.         0.         0.83333333 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.16666667 0.         0.33333333 0.         0.         0.16666667\n",
      "  0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.16666667 0.16666667 0.16666667\n",
      "  0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.16666667 0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.16666667 0.16666667 0.16666667\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.16666667 0.         0.         0.16666667 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.16666667 0.33333333\n",
      "  0.         0.16666667 0.         0.         0.         0.\n",
      "  0.16666667 0.         0.         0.16666667 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.16666667 0.         0.         0.33333333\n",
      "  0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.16666667]\n",
      " [0.         0.         0.16666667 0.         0.         0.16666667\n",
      "  0.         0.16666667 0.         0.16666667 0.         0.\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.16666667 0.         0.16666667\n",
      "  0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.16666667 0.         0.         0.16666667 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.33333333 0.16666667 0.         0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.16666667]\n",
      " [0.16666667 0.         0.33333333 0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.16666667 0.         0.         0.16666667\n",
      "  0.16666667 0.         0.16666667 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.16666667]\n",
      " [0.16666667 0.         0.33333333 0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.16666667 0.         0.16666667 0.         0.         0.16666667\n",
      "  0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.         0.        ]\n",
      " [0.16666667 0.         0.5        0.         0.         0.16666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.16666667 0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.16666667 0.         0.\n",
      "  0.16666667 0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.16666667 0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.16666667 0.         0.         0.16666667 0.         0.\n",
      "  0.         0.16666667]\n",
      " [0.33333333 0.         0.         0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.16666667]\n",
      " [0.5        0.16666667 0.         0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.16666667 0.         0.16666667 0.         0.         0.16666667\n",
      "  0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.         0.16666667 0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.16666667 0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.16666667 0.         0.\n",
      "  0.16666667 0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.16666667 0.         0.         0.         0.16666667 0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.16666667 0.         0.16666667 0.         0.         0.\n",
      "  0.16666667 0.        ]\n",
      " [0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.16666667 0.         0.         0.16666667 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.16666667 0.         0.         0.33333333\n",
      "  0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.16666667]\n",
      " [0.33333333 0.         0.         0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.16666667 0.         0.         0.         0.\n",
      "  0.         0.16666667]\n",
      " [0.         0.         0.16666667 0.         0.16666667 0.16666667\n",
      "  0.         0.16666667 0.         0.         0.         0.\n",
      "  0.16666667 0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.16666667 0.         0.\n",
      "  0.33333333 0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.         0.16666667 0.         0.16666667 0.         0.\n",
      "  0.16666667 0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.16666667 0.         0.\n",
      "  0.33333333 0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.14285714 0.14285714 0.14285714\n",
      "  0.         0.         0.         0.         0.14285714 0.14285714\n",
      "  0.14285714 0.         0.         0.14285714 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.14285714 0.         0.42857143 0.         0.         0.28571429\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.14285714 0.\n",
      "  0.         0.        ]\n",
      " [0.28571429 0.         0.14285714 0.         0.         0.14285714\n",
      "  0.         0.         0.14285714 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.14285714 0.14285714\n",
      "  0.         0.        ]\n",
      " [0.28571429 0.14285714 0.         0.         0.14285714 0.14285714\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.14285714 0.         0.         0.\n",
      "  0.         0.14285714]\n",
      " [0.         0.         0.         0.         0.         0.28571429\n",
      "  0.         0.28571429 0.         0.         0.         0.\n",
      "  0.28571429 0.         0.         0.         0.14285714 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.14285714 0.14285714\n",
      "  0.         0.         0.         0.14285714 0.         0.\n",
      "  0.28571429 0.         0.         0.         0.14285714 0.\n",
      "  0.14285714 0.        ]\n",
      " [0.         0.         0.         0.         0.14285714 0.14285714\n",
      "  0.         0.28571429 0.         0.         0.         0.\n",
      "  0.14285714 0.         0.         0.         0.14285714 0.14285714\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.14285714 0.28571429\n",
      "  0.         0.14285714 0.         0.         0.         0.\n",
      "  0.14285714 0.         0.         0.14285714 0.         0.\n",
      "  0.         0.14285714]\n",
      " [0.         0.         0.         0.14285714 0.14285714 0.14285714\n",
      "  0.         0.         0.         0.         0.28571429 0.\n",
      "  0.14285714 0.         0.         0.14285714 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.14285714 0.         0.         0.         0.14285714 0.\n",
      "  0.         0.         0.         0.         0.14285714 0.\n",
      "  0.14285714 0.14285714 0.14285714 0.         0.         0.\n",
      "  0.14285714 0.        ]\n",
      " [0.         0.         0.         0.         0.14285714 0.28571429\n",
      "  0.         0.14285714 0.         0.         0.         0.\n",
      "  0.14285714 0.         0.         0.14285714 0.         0.\n",
      "  0.         0.14285714]\n",
      " [0.         0.         0.         0.         0.14285714 0.14285714\n",
      "  0.         0.         0.         0.         0.14285714 0.14285714\n",
      "  0.14285714 0.         0.         0.14285714 0.         0.\n",
      "  0.         0.14285714]\n",
      " [0.         0.         0.14285714 0.         0.         0.28571429\n",
      "  0.         0.         0.14285714 0.         0.         0.14285714\n",
      "  0.         0.         0.         0.         0.14285714 0.\n",
      "  0.         0.14285714]\n",
      " [0.         0.14285714 0.14285714 0.         0.         0.28571429\n",
      "  0.         0.         0.14285714 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.14285714 0.\n",
      "  0.         0.14285714]\n",
      " [0.         0.         0.14285714 0.         0.         0.14285714\n",
      "  0.14285714 0.14285714 0.         0.14285714 0.         0.\n",
      "  0.         0.         0.         0.         0.28571429 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.14285714 0.14285714 0.14285714\n",
      "  0.         0.         0.         0.         0.14285714 0.14285714\n",
      "  0.14285714 0.         0.         0.14285714 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.28571429 0.14285714 0.         0.         0.14285714 0.14285714\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.14285714 0.         0.         0.\n",
      "  0.         0.14285714]\n",
      " [0.14285714 0.         0.42857143 0.         0.         0.28571429\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.14285714 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.14285714 0.14285714 0.         0.14285714\n",
      "  0.14285714 0.         0.14285714 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.14285714 0.\n",
      "  0.         0.14285714]\n",
      " [0.14285714 0.         0.42857143 0.         0.         0.28571429\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.14285714 0.\n",
      "  0.         0.        ]\n",
      " [0.28571429 0.         0.14285714 0.         0.         0.14285714\n",
      "  0.         0.         0.14285714 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.14285714 0.14285714\n",
      "  0.         0.        ]\n",
      " [0.14285714 0.         0.42857143 0.         0.         0.28571429\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.14285714 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.14285714 0.         0.14285714 0.14285714\n",
      "  0.         0.14285714 0.         0.14285714 0.         0.\n",
      "  0.14285714 0.         0.         0.         0.14285714 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.14285714 0.         0.14285714 0.14285714\n",
      "  0.         0.14285714 0.         0.14285714 0.         0.\n",
      "  0.14285714 0.         0.         0.         0.14285714 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.14285714 0.28571429\n",
      "  0.         0.14285714 0.         0.         0.         0.\n",
      "  0.14285714 0.         0.         0.14285714 0.         0.\n",
      "  0.         0.14285714]\n",
      " [0.42857143 0.         0.         0.         0.14285714 0.14285714\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.14285714 0.         0.         0.\n",
      "  0.         0.14285714]\n",
      " [0.42857143 0.14285714 0.         0.         0.14285714 0.14285714\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.14285714 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.14285714 0.14285714\n",
      "  0.         0.         0.         0.14285714 0.         0.\n",
      "  0.28571429 0.         0.         0.         0.14285714 0.\n",
      "  0.14285714 0.        ]\n",
      " [0.         0.         0.14285714 0.         0.14285714 0.14285714\n",
      "  0.         0.14285714 0.         0.14285714 0.         0.\n",
      "  0.14285714 0.         0.         0.         0.14285714 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.14285714 0.14285714 0.14285714\n",
      "  0.         0.         0.         0.         0.14285714 0.14285714\n",
      "  0.14285714 0.         0.         0.14285714 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.28571429 0.         0.14285714 0.         0.14285714 0.14285714\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.14285714 0.         0.         0.         0.\n",
      "  0.         0.14285714]\n",
      " [0.         0.         0.14285714 0.         0.14285714 0.14285714\n",
      "  0.         0.14285714 0.         0.14285714 0.         0.\n",
      "  0.14285714 0.         0.         0.         0.14285714 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.14285714\n",
      "  0.         0.28571429 0.         0.         0.         0.\n",
      "  0.28571429 0.         0.         0.         0.14285714 0.14285714\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.28571429\n",
      "  0.         0.28571429 0.         0.         0.         0.\n",
      "  0.14285714 0.         0.         0.         0.14285714 0.14285714\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.14285714\n",
      "  0.         0.28571429 0.         0.         0.         0.\n",
      "  0.28571429 0.         0.         0.         0.14285714 0.14285714\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.14285714 0.14285714 0.14285714\n",
      "  0.         0.         0.         0.         0.14285714 0.14285714\n",
      "  0.14285714 0.         0.         0.14285714 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.14285714 0.14285714 0.         0.         0.14285714 0.14285714\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.14285714 0.14285714 0.         0.         0.\n",
      "  0.         0.14285714]\n",
      " [0.14285714 0.14285714 0.         0.         0.14285714 0.14285714\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.14285714 0.14285714 0.         0.         0.\n",
      "  0.         0.14285714]\n",
      " [0.         0.         0.         0.125      0.125      0.25\n",
      "  0.         0.         0.         0.         0.125      0.125\n",
      "  0.125      0.         0.         0.125      0.         0.\n",
      "  0.         0.        ]\n",
      " [0.375      0.125      0.         0.         0.125      0.125\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.125      0.         0.         0.\n",
      "  0.         0.125     ]\n",
      " [0.         0.         0.         0.         0.125      0.25\n",
      "  0.         0.125      0.         0.125      0.         0.\n",
      "  0.375      0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.25       0.         0.375      0.         0.         0.25\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.125      0.\n",
      "  0.         0.        ]\n",
      " [0.125      0.         0.375      0.         0.         0.25\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.125      0.         0.125      0.\n",
      "  0.         0.        ]\n",
      " [0.125      0.         0.375      0.         0.         0.25\n",
      "  0.125      0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.125      0.\n",
      "  0.         0.        ]\n",
      " [0.25       0.         0.125      0.125      0.         0.125\n",
      "  0.         0.         0.125      0.         0.         0.\n",
      "  0.         0.         0.         0.         0.125      0.125\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.125      0.125\n",
      "  0.         0.         0.125      0.125      0.         0.\n",
      "  0.25       0.         0.         0.         0.125      0.\n",
      "  0.125      0.        ]\n",
      " [0.125      0.         0.375      0.         0.         0.25\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.125      0.125      0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.125      0.         0.125      0.125\n",
      "  0.         0.125      0.         0.125      0.125      0.\n",
      "  0.125      0.         0.         0.         0.125      0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.125      0.         0.125      0.125\n",
      "  0.         0.125      0.         0.125      0.         0.\n",
      "  0.125      0.         0.125      0.         0.125      0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.125      0.25\n",
      "  0.         0.125      0.         0.         0.         0.\n",
      "  0.125      0.         0.         0.125      0.         0.125\n",
      "  0.         0.125     ]\n",
      " [0.375      0.125      0.         0.         0.125      0.125\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.125      0.         0.         0.\n",
      "  0.         0.125     ]\n",
      " [0.375      0.         0.375      0.         0.         0.125\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.125      0.\n",
      "  0.         0.        ]\n",
      " [0.125      0.125      0.         0.         0.125      0.125\n",
      "  0.         0.         0.125      0.         0.         0.\n",
      "  0.         0.125      0.125      0.         0.         0.\n",
      "  0.         0.125     ]\n",
      " [0.25       0.125      0.         0.         0.125      0.125\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.125      0.125      0.         0.         0.\n",
      "  0.         0.125     ]\n",
      " [0.375      0.125      0.         0.         0.125      0.125\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.125      0.         0.         0.\n",
      "  0.         0.125     ]\n",
      " [0.         0.125      0.         0.         0.125      0.125\n",
      "  0.         0.         0.         0.125      0.         0.\n",
      "  0.25       0.         0.         0.         0.125      0.\n",
      "  0.125      0.        ]\n",
      " [0.22222222 0.         0.33333333 0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.11111111 0.\n",
      "  0.         0.        ]\n",
      " [0.44444444 0.         0.33333333 0.         0.         0.11111111\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.11111111 0.\n",
      "  0.         0.        ]\n",
      " [0.11111111 0.         0.33333333 0.         0.         0.22222222\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.11111111 0.         0.11111111 0.\n",
      "  0.11111111 0.        ]\n",
      " [0.11111111 0.         0.33333333 0.         0.         0.22222222\n",
      "  0.11111111 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.11111111 0.         0.11111111 0.\n",
      "  0.         0.        ]\n",
      " [0.22222222 0.         0.11111111 0.11111111 0.         0.11111111\n",
      "  0.         0.         0.11111111 0.         0.         0.\n",
      "  0.         0.         0.         0.11111111 0.11111111 0.11111111\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.11111111 0.11111111\n",
      "  0.         0.         0.11111111 0.11111111 0.11111111 0.\n",
      "  0.22222222 0.         0.         0.         0.11111111 0.\n",
      "  0.11111111 0.        ]\n",
      " [0.11111111 0.         0.33333333 0.         0.         0.22222222\n",
      "  0.         0.         0.         0.         0.         0.11111111\n",
      "  0.         0.         0.         0.11111111 0.11111111 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.11111111 0.22222222\n",
      "  0.11111111 0.11111111 0.         0.         0.         0.\n",
      "  0.11111111 0.         0.         0.11111111 0.         0.11111111\n",
      "  0.         0.11111111]\n",
      " [0.33333333 0.11111111 0.         0.         0.11111111 0.11111111\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.11111111 0.         0.11111111 0.\n",
      "  0.         0.11111111]\n",
      " [0.         0.         0.11111111 0.11111111 0.         0.11111111\n",
      "  0.11111111 0.         0.11111111 0.         0.         0.\n",
      "  0.         0.11111111 0.         0.         0.11111111 0.11111111\n",
      "  0.         0.11111111]\n",
      " [0.         0.11111111 0.11111111 0.         0.11111111 0.11111111\n",
      "  0.         0.11111111 0.         0.11111111 0.11111111 0.\n",
      "  0.11111111 0.         0.         0.         0.11111111 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.11111111 0.         0.11111111 0.11111111\n",
      "  0.         0.11111111 0.11111111 0.11111111 0.         0.\n",
      "  0.11111111 0.         0.11111111 0.         0.11111111 0.\n",
      "  0.         0.        ]\n",
      " [0.33333333 0.11111111 0.         0.         0.11111111 0.11111111\n",
      "  0.11111111 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.11111111 0.         0.         0.\n",
      "  0.         0.11111111]\n",
      " [0.2        0.         0.3        0.         0.1        0.3\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.1        0.\n",
      "  0.         0.        ]\n",
      " [0.1        0.         0.2        0.         0.         0.5\n",
      "  0.         0.         0.1        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.1        0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.1        0.1        0.4\n",
      "  0.         0.         0.         0.         0.1        0.1\n",
      "  0.1        0.         0.         0.1        0.         0.\n",
      "  0.         0.        ]\n",
      " [0.6        0.         0.         0.         0.1        0.1\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.1        0.         0.         0.\n",
      "  0.         0.1       ]\n",
      " [0.09090909 0.         0.18181818 0.         0.         0.54545455\n",
      "  0.         0.         0.09090909 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.09090909 0.\n",
      "  0.         0.        ]\n",
      " [0.36363636 0.         0.27272727 0.         0.         0.27272727\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.09090909 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.08333333 0.08333333 0.41666667\n",
      "  0.         0.         0.         0.         0.16666667 0.08333333\n",
      "  0.08333333 0.         0.         0.08333333 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.08333333 0.08333333 0.5\n",
      "  0.         0.         0.         0.         0.08333333 0.08333333\n",
      "  0.08333333 0.         0.         0.08333333 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.25       0.         0.         0.         0.25       0.25\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.25       0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.58333333 0.08333333 0.         0.         0.08333333 0.08333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.08333333 0.         0.         0.         0.\n",
      "  0.         0.08333333]\n",
      " [0.         0.         0.25       0.         0.08333333 0.33333333\n",
      "  0.         0.         0.08333333 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.16666667 0.         0.16666667 0.16666667\n",
      "  0.         0.16666667 0.         0.16666667 0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.5        0.08333333 0.         0.         0.08333333 0.08333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.08333333 0.08333333 0.         0.         0.\n",
      "  0.         0.08333333]\n",
      " [0.         0.         0.         0.07692308 0.07692308 0.46153846\n",
      "  0.         0.         0.         0.         0.15384615 0.07692308\n",
      "  0.07692308 0.         0.         0.07692308 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.30769231 0.         0.23076923 0.         0.         0.38461538\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.07692308 0.\n",
      "  0.         0.        ]\n",
      " [0.25       0.         0.25       0.         0.         0.25\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.23076923 0.         0.07692308 0.23076923\n",
      "  0.         0.07692308 0.         0.15384615 0.         0.\n",
      "  0.07692308 0.         0.         0.         0.15384615 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.07142857 0.07142857 0.5\n",
      "  0.         0.         0.         0.         0.14285714 0.07142857\n",
      "  0.07142857 0.         0.         0.07142857 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.21428571 0.         0.07142857 0.42857143\n",
      "  0.         0.         0.07142857 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.21428571 0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Amino acid composition (AAC)\n",
    "handcraft_AAC_test = [[0] * 20 for _ in range(len(train1_oe1))]\n",
    "for row in range(len(train1_oe1)):\n",
    "    seq = train1_oe1[row]\n",
    "    for i in seq:\n",
    "        col = i[0]-1\n",
    "        handcraft_AAC_test[row][col] += 1/len(seq)\n",
    "hc_AAC_test = np.array(handcraft_AAC_test)\n",
    "print(hc_AAC_test.shape)\n",
    "print(hc_AAC_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e99d8ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(306, 400)\n",
      "Length of feature vector: 400\n"
     ]
    }
   ],
   "source": [
    "#Dipeptide composition (DPC)\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def compute_dpc_pairs(sequence, k):\n",
    "    return [sequence[i] + sequence[i + k + 1] for i in range(len(sequence) - k - 1)]\n",
    "\n",
    "def calculate_amino_acid_pairs_frequency(sequence, max_k):\n",
    "    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "    pair_count = len(amino_acids) ** 2\n",
    "    feature_vector = []\n",
    "\n",
    "    for k in range(max_k + 1):\n",
    "        dpc_pairs = compute_dpc_pairs(sequence, k)\n",
    "        pair_counter = Counter(dpc_pairs)\n",
    "        total_pairs = len(dpc_pairs)\n",
    "\n",
    "        vector = [pair_counter.get(a + b, 0) / total_pairs for a in amino_acids for b in amino_acids]\n",
    "        feature_vector.extend(vector)\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "max_k = 0\n",
    "dpc_group_pairs = [calculate_amino_acid_pairs_frequency(sequence, max_k) for sequence in train1.values()]\n",
    "DPC = np.array(dpc_group_pairs)\n",
    "\n",
    "print(DPC.shape)\n",
    "print(\"Length of feature vector:\", len(dpc_group_pairs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b6ff86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(306, 280)\n"
     ]
    }
   ],
   "source": [
    "#The One-Hot descriptor for sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "amino_index = {aa: i for i, aa in enumerate(amino_acids)}\n",
    "max_length = max(df['Sequence'].apply(len))\n",
    "\n",
    "def sequence_to_one_hot(seq):\n",
    "    one_hot = np.zeros(max_length*20)\n",
    "    \n",
    "    for i, aa in enumerate(seq):\n",
    "        if i >= 14:\n",
    "            break\n",
    "        if aa in amino_index:\n",
    "            index = amino_index[aa] + i * 20\n",
    "            one_hot[index] = 1\n",
    "            \n",
    "    return one_hot\n",
    "\n",
    "sequences = df['Sequence']\n",
    "\n",
    "one_hot_encoded = np.array([sequence_to_one_hot(seq) for seq in sequences])\n",
    "\n",
    "print(one_hot_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d057ce4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated featuresPC: [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 0], [0, 0], [0, 0], [0, 1], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 0], [1, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 0], [1, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 0], [1, 0], [1, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 1], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
      "Generated featuresF: [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 1, 1], [0, 0, 1, 1], [0, 0, 1, 1], [0, 0, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 1, 1], [0, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [1, 0, 1, 1], [0, 0, 0, 0], [0, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [1, 0, 1, 1], [1, 0, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
      "Generated featuresM: [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [1, 0, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [1, 0, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [1, 0, 1], [0, 0, 0], [0, 0, 0], [1, 0, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [1, 0, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [1, 0, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [1, 1, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [1, 1, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [1, 1, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [1, 1, 1], [1, 1, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [1, 1, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [1, 1, 1], [0, 0, 0]]\n",
      "Generated featuresT: [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 1], [1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 1, 1], [1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 1], [1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 1], [0, 0, 0, 0, 1, 1], [1, 0, 0, 0, 1, 1], [1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 1], [1, 0, 0, 0, 1, 1], [1, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 1], [0, 0, 1, 0, 0, 1], [0, 0, 1, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 1, 1], [0, 0, 1, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 1, 1], [1, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 1, 1], [1, 1, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1], [1, 1, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 1], [1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 1, 1], [1, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 1, 1]]\n",
      "Generated featuresG: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "(306, 16)\n"
     ]
    }
   ],
   "source": [
    "#Hand-crafted features\n",
    "def summarized_featuresPC(peptide_sequences):\n",
    "    summarized_featuresPC = []\n",
    "    for i in peptide_sequences:\n",
    "        last_aa = peptide_sequences[i][0]  # Get the last amino acid\n",
    "        if last_aa == 'P':\n",
    "            summarized_featuresPC.append([1,0])  #If the last amino acid is Pro\n",
    "        elif last_aa == 'C':\n",
    "            summarized_featuresPC.append([0,1])\n",
    "        else:\n",
    "            summarized_featuresPC.append([0,0])\n",
    "    return summarized_featuresPC\n",
    "def summarized_featuresF(peptide_sequences):\n",
    "    summarized_featuresF = []\n",
    "    for i in peptide_sequences:\n",
    "        last_aa = peptide_sequences[i][0]\n",
    "        first_aa = peptide_sequences[i][-1]\n",
    "        first3_aa = peptide_sequences[i][-3:]\n",
    "        if last_aa == 'F':\n",
    "            if first3_aa == 'CRG':\n",
    "                summarized_featuresF.append([1,0,1,1])\n",
    "            elif first3_aa == 'YRG':\n",
    "                summarized_featuresF.append([0,1,1,1])\n",
    "            elif first_aa == 'G':\n",
    "                summarized_featuresF.append([0,0,1,1])\n",
    "            else:\n",
    "                summarized_featuresF.append([0,0,0,1])\n",
    "        else:\n",
    "            summarized_featuresF.append([0,0,0,0])\n",
    "    return summarized_featuresF\n",
    "def summarized_featuresM(peptide_sequences):\n",
    "    summarized_featuresM = []\n",
    "    for i in peptide_sequences:\n",
    "        last_aa = peptide_sequences[i][0]\n",
    "        first_aa = peptide_sequences[i][-1]\n",
    "        first6_aa = peptide_sequences[i][-6:]\n",
    "        if last_aa == 'M':\n",
    "            if first6_aa == 'PNSFEG':\n",
    "                summarized_featuresM.append([1,1,1])\n",
    "            elif first_aa == 'G':\n",
    "                summarized_featuresM.append([1,0,1])\n",
    "            else:\n",
    "                summarized_featuresM.append([0,0,1])\n",
    "        else:\n",
    "            summarized_featuresM.append([0,0,0])\n",
    "    return summarized_featuresM\n",
    "def summarized_featuresT(peptide_sequences):\n",
    "    summarized_featuresT = []\n",
    "    for i in peptide_sequences:\n",
    "        last_aa = peptide_sequences[i][0]\n",
    "        last2_aa = peptide_sequences[i][0:2]\n",
    "        first_aa = peptide_sequences[i][-1]\n",
    "        first2_aa = peptide_sequences[i][-2:]\n",
    "        if last_aa == 'T':\n",
    "            if last2_aa == 'TD':\n",
    "                if first2_aa == 'GG':\n",
    "                    summarized_featuresT.append([1,1,0,0,1,1])\n",
    "                elif first_aa == 'G':\n",
    "                    summarized_featuresT.append([1,0,0,0,1,1])\n",
    "                else:\n",
    "                    summarized_featuresT.append([0,0,0,0,1,1])\n",
    "            elif first2_aa == 'DG':\n",
    "                summarized_featuresT.append([0,0,1,0,0,1])\n",
    "            elif first2_aa == 'FG':\n",
    "                summarized_featuresT.append([0,0,0,1,0,1])\n",
    "            elif first_aa == 'G':\n",
    "                summarized_featuresT.append([1,0,0,0,0,1])\n",
    "            else:\n",
    "                summarized_featuresT.append([0,0,0,0,0,1])\n",
    "        else:\n",
    "            summarized_featuresT.append([0,0,0,0,0,0])\n",
    "    return summarized_featuresT\n",
    "def summarized_featuresG(peptide_sequences):\n",
    "    summarized_featuresG = []\n",
    "    for i in peptide_sequences:\n",
    "        last_aa = peptide_sequences[i][0]\n",
    "        if last_aa == 'G':\n",
    "            summarized_featuresG.append(1)\n",
    "        else:\n",
    "            summarized_featuresG.append(0)\n",
    "    return summarized_featuresG\n",
    "\n",
    "\n",
    "featuresPC = summarized_featuresPC(train1)\n",
    "featuresF = summarized_featuresF(train1)\n",
    "featuresM = summarized_featuresM(train1)\n",
    "featuresT = summarized_featuresT(train1)\n",
    "featuresG = summarized_featuresG(train1)\n",
    "\n",
    "\n",
    "print(\"Generated featuresPC:\", featuresPC)\n",
    "print(\"Generated featuresF:\", featuresF)\n",
    "print(\"Generated featuresM:\", featuresM)\n",
    "print(\"Generated featuresT:\", featuresT)\n",
    "print(\"Generated featuresG:\", featuresG)\n",
    "Generated_features = np.c_[featuresPC,featuresF,featuresM,featuresT,featuresG]\n",
    "print(Generated_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff16449f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove: [12, 39, 42, 45, 47, 48, 50, 51, 52, 54, 57, 58, 59, 60, 62, 63, 67, 68, 69, 71, 72, 73, 79, 83, 84, 86, 87, 89, 90, 91, 93, 94, 97, 99, 103, 104, 105, 107, 109, 113, 114, 115, 118, 122, 126, 127, 131, 133, 135, 137, 139, 147, 151, 153, 154, 156, 159, 162, 163, 165, 166, 167, 168, 169, 171, 174, 176, 179, 182, 183, 184, 186, 187, 190, 191, 193, 194, 195, 202, 203, 205, 206, 207, 208, 209, 211, 215, 216, 217, 219, 220, 222, 223, 224, 225, 226, 227, 229, 231, 232, 233, 235, 237, 238, 239, 240, 242, 246, 249, 252, 253, 254, 256, 257, 258, 259, 262, 265, 267, 268, 269, 270, 272, 273, 274, 276, 277, 279, 284, 285, 289, 290, 294, 295, 297, 298, 299, 300, 302, 304, 305, 307, 309, 310, 311, 312, 316, 319, 320, 323, 325, 326, 327, 329, 330, 332, 334, 335, 336, 337, 338, 342, 343, 344, 345, 349, 350, 351, 354, 359, 366, 367, 369, 371, 372, 373, 374, 376, 377, 382, 384, 385, 387, 388, 390, 391, 392, 393, 395, 397, 398, 399, 400, 402, 404, 405, 406, 409, 411, 412, 413, 414, 418, 419, 423, 426, 428, 431, 434, 519, 542, 549, 553, 573, 579, 582, 590, 591, 593, 594, 597, 602, 603, 606, 609, 613, 614, 616, 619, 622, 623, 624, 625, 626, 628, 629, 633, 634, 635, 642, 643, 644, 646, 647, 648, 649, 650, 653, 654, 655, 656, 657, 662, 663, 664, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 680, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['scaler307.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "X_train = np.c_[Generated_features, hc_AAC_test, DPC, one_hot_encoded]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler = scaler.fit(X_train)\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "df2 = pd.DataFrame(X_scaled)\n",
    "df2_non_zero = df2.loc[:, (df2 != 0).any(axis=0)]\n",
    "removed_indices = df2.columns[~df2.columns.isin(df2_non_zero.columns)].tolist()\n",
    "print(\"Remove:\", removed_indices)\n",
    "X_filtered=np.array(df2_non_zero)\n",
    "joblib.dump(scaler, \"scaler307.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56537282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
      "174\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "y_train1=[]\n",
    "count1=0\n",
    "count0=0\n",
    "for idx in df.index:\n",
    "    y_train1 += [df['Score'].loc[idx]] \n",
    "    if df['Score'].loc[idx]==1:\n",
    "        count1+=1\n",
    "    else:\n",
    "        count0+=1\n",
    "y_train1 = np.array(y_train1, dtype = float)\n",
    "print(y_train1)\n",
    "print(count1)\n",
    "print(count0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e16c2e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[243  39  84   1 134 217 300 273 276 185  73 262 122   5  46 174  36 158\n",
      "  52 152 210 133 101 108   7  83 291 129 274 189 303  82 279  59  29  34\n",
      " 164 252  43 254 192 136  51 200 183 227  20  15 261  23  25 105 205 302\n",
      " 176 194 266 203  21 209 301 223 201 110  30 155 159 116 123 222 127 187\n",
      "   8  67 139  41 290  79  85  44  93  71 268 154  10  90 204 295  69 141\n",
      "  50 214 229 118 168 135  48 115 153  72 126 172  57 149 144   9  98  13\n",
      " 207 211 246  74 213 267 245 240 146 182  64 226  53 173 143 293  33  75\n",
      "   6 286 230 269  99  11  63  91 255  22 191  68  76  96 177  94 128  95\n",
      " 239 150 138 218 145 250 297 117  16 280 188 119 169 249 131 281  24 219\n",
      "  47 196 206 113 231 179  28  80 142 260 271 170 285 277 140  65  62 130\n",
      "  60 107 292  78   3 208 258  17 157 106 298 299 181  42 114  32 263  27\n",
      "  49 264 175 242 287 237  45 221 294 278 289 100  26  35 224 120]\n",
      "[ 12  66 256 184 235   0 163 272 166 186 199 193 156 296  31 284  87 241\n",
      " 167 102   4 215 178 234 305 282 304 202 165 228 162  97 232 171  77  58\n",
      " 220 132 251 283 197 125 104 160 244 236 257  18 112  19 238 225  81  55\n",
      " 248 161   2 198 212 103  92 111 265 147 247 151  40 275  14 233 180 216\n",
      " 137  61 109  38 121 124 148 259  56 253  37 190 270  86  54 195  89  70\n",
      " 288  88]\n",
      "122\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "for train_index, test_index in sss.split(X_filtered, y_train1):\n",
    "    x_train, x_test = X_filtered[train_index], X_filtered[test_index]\n",
    "    y_train, y_test = y_train1[train_index], y_train1[test_index]\n",
    "print(train_index)\n",
    "print(test_index)\n",
    "train1=0\n",
    "train0=0\n",
    "for i in y_train:\n",
    "    if i==1:\n",
    "        train1+=1\n",
    "    else:\n",
    "        train0+=1\n",
    "print(train1)\n",
    "print(train0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6e70009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GaussianNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb_classifier = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca631b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5, p= 1, weights='distance')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb41488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "modelGB = GradientBoostingClassifier(n_estimators=100, learning_rate=0.5, max_depth=7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "389aed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "modelRF = RandomForestClassifier( random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c3fb9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "modelSVC = SVC(random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29426582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "modelLR = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55415ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGB\n",
    "import xgboost as xgb\n",
    "xgb_clf = xgb.XGBClassifier(colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43d271c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "base_model = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "adaboost_clf = AdaBoostClassifier(base_estimator=base_model, n_estimators=200, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15c03c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "clf1 = GaussianNB()\n",
    "clf2 = RandomForestClassifier(random_state=42)\n",
    "clf3 = SVC(random_state=42, probability=True)\n",
    "clf4 = xgb.XGBClassifier(colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0, random_state=42)\n",
    "voting_clf1 = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('svc', clf3), ('xgb', clf4)], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d67bbb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = RandomForestClassifier(random_state=42)\n",
    "clf2 = SVC(random_state=42, probability=True)\n",
    "voting_clf2 = VotingClassifier(estimators=[('rf', clf1), ('svm', clf2)], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "688af0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = GaussianNB()\n",
    "clf2 = RandomForestClassifier( random_state=42)\n",
    "clf3 = SVC(random_state=42, probability=True)\n",
    "voting_clf3 = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('svc', clf3)], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66195af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier \n",
    "stack1 = GaussianNB()\n",
    "stack2 = RandomForestClassifier( random_state=42)\n",
    "stack3 = SVC(random_state=42, probability=True)\n",
    "stack = StackingClassifier(estimators=[( 'lr' , stack1), ( 'RF' , stack2), ( 'SVC' , stack3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84dc2f15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating GaussianNB...\n",
      "Average accuracy: 0.6930\n",
      "Average precision: 0.7192\n",
      "Average recall: 0.7435\n",
      "Average f1: 0.7286\n",
      "\n",
      "\n",
      "Evaluating KNeighborsClassifier...\n",
      "Average accuracy: 0.6570\n",
      "Average precision: 0.7443\n",
      "Average recall: 0.6023\n",
      "Average f1: 0.6604\n",
      "\n",
      "\n",
      "Evaluating GradientBoostingClassifier...\n",
      "Average accuracy: 0.6764\n",
      "Average precision: 0.7084\n",
      "Average recall: 0.7269\n",
      "Average f1: 0.7153\n",
      "\n",
      "\n",
      "Evaluating RandomForestClassifier...\n",
      "Average accuracy: 0.6960\n",
      "Average precision: 0.7299\n",
      "Average recall: 0.7421\n",
      "Average f1: 0.7327\n",
      "\n",
      "\n",
      "Evaluating SVC...\n",
      "Average accuracy: 0.7124\n",
      "Average precision: 0.7186\n",
      "Average recall: 0.8099\n",
      "Average f1: 0.7600\n",
      "\n",
      "\n",
      "Evaluating LogisticRegression...\n",
      "Average accuracy: 0.6797\n",
      "Average precision: 0.7102\n",
      "Average recall: 0.7348\n",
      "Average f1: 0.7209\n",
      "\n",
      "\n",
      "Evaluating XGBClassifier...\n",
      "[15:23:49] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:23:49] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:23:49] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:23:50] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:23:50] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Average accuracy: 0.6275\n",
      "Average precision: 0.6747\n",
      "Average recall: 0.6436\n",
      "Average f1: 0.6581\n",
      "\n",
      "\n",
      "Evaluating AdaBoostClassifier...\n",
      "Average accuracy: 0.6570\n",
      "Average precision: 0.6981\n",
      "Average recall: 0.6937\n",
      "Average f1: 0.6926\n",
      "\n",
      "\n",
      "Evaluating VotingClassifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:23:54] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:23:54] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:23:55] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:23:55] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:23:56] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Average accuracy: 0.7092\n",
      "Average precision: 0.7294\n",
      "Average recall: 0.7631\n",
      "Average f1: 0.7449\n",
      "\n",
      "\n",
      "Evaluating VotingClassifier...\n",
      "Average accuracy: 0.6926\n",
      "Average precision: 0.7050\n",
      "Average recall: 0.7931\n",
      "Average f1: 0.7441\n",
      "\n",
      "\n",
      "Evaluating VotingClassifier...\n",
      "Average accuracy: 0.7060\n",
      "Average precision: 0.7274\n",
      "Average recall: 0.7623\n",
      "Average f1: 0.7426\n",
      "\n",
      "\n",
      "Evaluating StackingClassifier...\n",
      "Average accuracy: 0.6012\n",
      "Average precision: 0.6133\n",
      "Average recall: 0.8595\n",
      "Average f1: 0.7088\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "models = [nb_classifier, knn_classifier, modelGB, modelRF, modelSVC, modelLR, xgb_clf, adaboost_clf, voting_clf1, voting_clf2, voting_clf3,stack]  # 假设这里有三个模型\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for model in models:\n",
    "    print(f\"Evaluating {model.__class__.__name__}...\")\n",
    "\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    for train_index, test_index in kfold.split(X_scaled, y_train1):\n",
    "        x_train, x_test = X_filtered[train_index], X_filtered[test_index]\n",
    "        y_train, y_test = y_train1[train_index], y_train1[test_index]\n",
    "\n",
    "        \n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        \n",
    "        y_pred = model.predict(x_test)\n",
    "\n",
    "       \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        precisions.append(precision)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        recalls.append(recall)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        f1s.append(f1)\n",
    "  \n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    mean_precision = np.mean(precisions)\n",
    "    mean_recall = np.mean(recalls)\n",
    "    mean_f1 = np.mean(f1s)\n",
    "    print(f\"Average accuracy: {mean_accuracy:.4f}\")\n",
    "    print(f\"Average precision: {mean_precision:.4f}\")\n",
    "    print(f\"Average recall: {mean_recall:.4f}\")\n",
    "    print(f\"Average f1: {mean_f1:.4f}\")\n",
    "\n",
    "  \n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1dad1a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6077736647276573 [28]\n",
      "0.6405605499735589 [28, 32]\n",
      "0.6635642517186674 [28, 32, 0]\n",
      "0.6865150713907986 [28, 32, 0, 24]\n",
      "0.7060285563194078 [28, 32, 0, 24, 21]\n",
      "0.7255949233209942 [28, 32, 0, 24, 21, 18]\n",
      "0.7452141723955579 [28, 32, 0, 24, 21, 18, 22]\n",
      "0.7517715494447383 [28, 32, 0, 24, 21, 18, 22, 39]\n",
      "0.7615547329455314 [28, 32, 0, 24, 21, 18, 22, 39, 56]\n",
      "0.7680592279217345 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296]\n",
      "0.7745637228979376 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367]\n",
      "0.7810682178741406 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396]\n",
      "0.7843997884717081 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316]\n",
      "0.7876784769962983 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58]\n",
      "0.7909571655208885 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147]\n",
      "0.7909571655208885 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147, 153]\n",
      "0.7942358540454786 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147, 153, 85]\n",
      "0.7975145425700687 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147, 153, 85, 144]\n",
      "0.8007932310946589 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147, 153, 85, 144, 207]\n",
      "0.8040719196192491 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147, 153, 85, 144, 207, 70]\n",
      "0.8105235325224749 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147, 153, 85, 144, 207, 70, 1]\n",
      "0.813802221047065 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147, 153, 85, 144, 207, 70, 1, 42]\n",
      "0.8203595980962453 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147, 153, 85, 144, 207, 70, 1, 42, 146]\n",
      "0.8236382866208356 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147, 153, 85, 144, 207, 70, 1, 42, 146, 123]\n",
      "0.8269169751454257 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147, 153, 85, 144, 207, 70, 1, 42, 146, 123, 385]\n",
      "0.8269169751454257 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147, 153, 85, 144, 207, 70, 1, 42, 146, 123, 385, 78]\n",
      "0.8302485457429931 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147, 153, 85, 144, 207, 70, 1, 42, 146, 123, 385, 78, 217]\n",
      "0.8335272342675832 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147, 153, 85, 144, 207, 70, 1, 42, 146, 123, 385, 78, 217, 121]\n",
      "0.8368059227921734 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147, 153, 85, 144, 207, 70, 1, 42, 146, 123, 385, 78, 217, 121, 174]\n",
      "0.8400846113167637 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147, 153, 85, 144, 207, 70, 1, 42, 146, 123, 385, 78, 217, 121, 174, 213]\n",
      "0.8433632998413538 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147, 153, 85, 144, 207, 70, 1, 42, 146, 123, 385, 78, 217, 121, 174, 213, 57]\n",
      "0.846641988365944 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147, 153, 85, 144, 207, 70, 1, 42, 146, 123, 385, 78, 217, 121, 174, 213, 57, 77]\n",
      "0.8498677948175569 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147, 153, 85, 144, 207, 70, 1, 42, 146, 123, 385, 78, 217, 121, 174, 213, 57, 77, 61]\n",
      "0.853146483342147 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147, 153, 85, 144, 207, 70, 1, 42, 146, 123, 385, 78, 217, 121, 174, 213, 57, 77, 61, 188]\n",
      "0.8564251718667372 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147, 153, 85, 144, 207, 70, 1, 42, 146, 123, 385, 78, 217, 121, 174, 213, 57, 77, 61, 188, 208]\n",
      "0.8596509783183499 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147, 153, 85, 144, 207, 70, 1, 42, 146, 123, 385, 78, 217, 121, 174, 213, 57, 77, 61, 188, 208, 180]\n",
      "0.8596509783183501 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147, 153, 85, 144, 207, 70, 1, 42, 146, 123, 385, 78, 217, 121, 174, 213, 57, 77, 61, 188, 208, 180, 317]\n",
      "0.8596509783183501 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147, 153, 85, 144, 207, 70, 1, 42, 146, 123, 385, 78, 217, 121, 174, 213, 57, 77, 61, 188, 208, 180, 317, 405]\n",
      "0.8596509783183501 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147, 153, 85, 144, 207, 70, 1, 42, 146, 123, 385, 78, 217, 121, 174, 213, 57, 77, 61, 188, 208, 180, 317, 405, 55]\n",
      "0.8596509783183501 [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147, 153, 85, 144, 207, 70, 1, 42, 146, 123, 385, 78, 217, 121, 174, 213, 57, 77, 61, 188, 208, 180, 317, 405, 55, 68]\n"
     ]
    }
   ],
   "source": [
    "def feature_selection(X, y):\n",
    "    remain_list = []  # List of selected feature indices\n",
    "    all_list = list(range(len(X[0])))  # Indices of all features\n",
    "    max_iterations = 40  # Set maximum iterations\n",
    "    iteration_count = 0 \n",
    "\n",
    "    for idx in range(40):  # Stop after 70 iterations\n",
    "        all_r2 = []\n",
    "        for id, each in enumerate(all_list):\n",
    "            print(f\"{id}/{len(all_list)}\", end='\\r')  # Display current progress\n",
    "\n",
    "            if each in remain_list:  # Skip if feature is already selected\n",
    "                all_r2.append(-10)\n",
    "                continue\n",
    "\n",
    "            temp_remain_list = remain_list + [each]  # Add the current feature to the candidate list\n",
    "            X_new = X[:, temp_remain_list]  # Use the selected feature subset\n",
    "            kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            model = modelSVC\n",
    "            acc = cross_val_score(model, X_new, y, cv=kfold, n_jobs=-1).mean()  # Calculate accuracy\n",
    "            all_r2.append(acc.mean())  # Add current feature's accuracy to the list\n",
    "\n",
    "        max_id = np.argmax(all_r2)  # Index of the feature with the maximum accuracy\n",
    "        remain_list.append(all_list[max_id])  # Add that feature to the selected list\n",
    "\n",
    "        print(np.max(all_r2), remain_list)  # Print the maximum accuracy and selected features\n",
    "\n",
    "        iteration_count += 1 \n",
    "\n",
    "    return all_r2, remain_list\n",
    "\n",
    "all_r2, y_in_removed_lists = feature_selection(X_filtered, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9347b467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[243  39  84   1 134 217 300 273 276 185  73 262 122   5  46 174  36 158\n",
      "  52 152 210 133 101 108   7  83 291 129 274 189 303  82 279  59  29  34\n",
      " 164 252  43 254 192 136  51 200 183 227  20  15 261  23  25 105 205 302\n",
      " 176 194 266 203  21 209 301 223 201 110  30 155 159 116 123 222 127 187\n",
      "   8  67 139  41 290  79  85  44  93  71 268 154  10  90 204 295  69 141\n",
      "  50 214 229 118 168 135  48 115 153  72 126 172  57 149 144   9  98  13\n",
      " 207 211 246  74 213 267 245 240 146 182  64 226  53 173 143 293  33  75\n",
      "   6 286 230 269  99  11  63  91 255  22 191  68  76  96 177  94 128  95\n",
      " 239 150 138 218 145 250 297 117  16 280 188 119 169 249 131 281  24 219\n",
      "  47 196 206 113 231 179  28  80 142 260 271 170 285 277 140  65  62 130\n",
      "  60 107 292  78   3 208 258  17 157 106 298 299 181  42 114  32 263  27\n",
      "  49 264 175 242 287 237  45 221 294 278 289 100  26  35 224 120]\n",
      "[ 12  66 256 184 235   0 163 272 166 186 199 193 156 296  31 284  87 241\n",
      " 167 102   4 215 178 234 305 282 304 202 165 228 162  97 232 171  77  58\n",
      " 220 132 251 283 197 125 104 160 244 236 257  18 112  19 238 225  81  55\n",
      " 248 161   2 198 212 103  92 111 265 147 247 151  40 275  14 233 180 216\n",
      " 137  61 109  38 121 124 148 259  56 253  37 190 270  86  54 195  89  70\n",
      " 288  88]\n",
      "122\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "X_new = X_filtered[:, [28, 32, 0, 24, 21, 18, 22, 39, 56, 296, 367, 396, 316, 58, 147, 153, 85, 144, 207, 70, 1, 42, 146, 123, 385, 78, 217, 121, 174, 213, 57, 77, 61, 188, 208, 180, 317, 405, 55, 68]]\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "for train_index, test_index in sss.split(X_new, y_train1):\n",
    "    x_train, x_test = X_new[train_index], X_new[test_index]\n",
    "    y_train, y_test = y_train1[train_index], y_train1[test_index]\n",
    "print(train_index)\n",
    "print(test_index)\n",
    "train1=0\n",
    "train0=0\n",
    "for i in y_train:\n",
    "    if i==1:\n",
    "        train1+=1\n",
    "    else:\n",
    "        train0+=1\n",
    "print(train1)\n",
    "print(train0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6392832e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0.\n",
      " 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
      "[1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
      "Accuracy: 0.8369565217391305\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.75      0.80        40\n",
      "         1.0       0.82      0.90      0.86        52\n",
      "\n",
      "    accuracy                           0.84        92\n",
      "   macro avg       0.84      0.83      0.83        92\n",
      "weighted avg       0.84      0.84      0.84        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "modelSVC = SVC()\n",
    "\n",
    "modelSVC.fit(x_train, y_train)\n",
    "\n",
    "y_test_SVC = modelSVC.predict(x_test)\n",
    "print(y_test_SVC)\n",
    "print(y_test)\n",
    "\n",
    "accuracy_SVC = accuracy_score(y_test, y_test_SVC)\n",
    "print(\"Accuracy:\", accuracy_SVC)\n",
    "\n",
    "print(classification_report(y_test, y_test_SVC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4e72e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_307_SVC.pkl']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(modelSVC, 'model_307_SVC.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "496b835f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating GaussianNB...\n",
      "Average accuracy: 0.6079\n",
      "Average precision: 0.5965\n",
      "Average recall: 0.9660\n",
      "Average f1: 0.7347\n",
      "\n",
      "\n",
      "Evaluating KNeighborsClassifier...\n",
      "Average accuracy: 0.6928\n",
      "Average precision: 0.7051\n",
      "Average recall: 0.7834\n",
      "Average f1: 0.7373\n",
      "\n",
      "\n",
      "Evaluating GradientBoostingClassifier...\n",
      "Average accuracy: 0.6994\n",
      "Average precision: 0.7042\n",
      "Average recall: 0.8156\n",
      "Average f1: 0.7528\n",
      "\n",
      "\n",
      "Evaluating RandomForestClassifier...\n",
      "Average accuracy: 0.7157\n",
      "Average precision: 0.7148\n",
      "Average recall: 0.8305\n",
      "Average f1: 0.7655\n",
      "\n",
      "\n",
      "Evaluating SVC...\n",
      "Average accuracy: 0.8597\n",
      "Average precision: 0.8764\n",
      "Average recall: 0.8821\n",
      "Average f1: 0.8781\n",
      "\n",
      "\n",
      "Evaluating LogisticRegression...\n",
      "Average accuracy: 0.6276\n",
      "Average precision: 0.6249\n",
      "Average recall: 0.8944\n",
      "Average f1: 0.7304\n",
      "\n",
      "\n",
      "Evaluating XGBClassifier...\n",
      "[15:42:51] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:42:51] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:42:51] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:42:52] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:42:52] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Average accuracy: 0.6962\n",
      "Average precision: 0.6878\n",
      "Average recall: 0.8585\n",
      "Average f1: 0.7607\n",
      "\n",
      "\n",
      "Evaluating AdaBoostClassifier...\n",
      "Average accuracy: 0.6895\n",
      "Average precision: 0.6950\n",
      "Average recall: 0.8093\n",
      "Average f1: 0.7453\n",
      "\n",
      "\n",
      "Evaluating VotingClassifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:42:54] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:42:55] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:42:55] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:42:55] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:42:56] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Average accuracy: 0.7224\n",
      "Average precision: 0.7016\n",
      "Average recall: 0.8995\n",
      "Average f1: 0.7858\n",
      "\n",
      "\n",
      "Evaluating VotingClassifier...\n",
      "Average accuracy: 0.7942\n",
      "Average precision: 0.7979\n",
      "Average recall: 0.8569\n",
      "Average f1: 0.8238\n",
      "\n",
      "\n",
      "Evaluating VotingClassifier...\n",
      "Average accuracy: 0.6832\n",
      "Average precision: 0.6695\n",
      "Average recall: 0.8880\n",
      "Average f1: 0.7596\n",
      "\n",
      "\n",
      "Evaluating StackingClassifier...\n",
      "Average accuracy: 0.8368\n",
      "Average precision: 0.8432\n",
      "Average recall: 0.8821\n",
      "Average f1: 0.8607\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "models = [nb_classifier, knn_classifier, modelGB, modelRF, modelSVC, modelLR, xgb_clf, adaboost_clf, voting_clf1, voting_clf2, voting_clf3,stack]  # 假设这里有三个模型\n",
    "\n",
    "# 定义 k 折交叉验证\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for model in models:\n",
    "    print(f\"Evaluating {model.__class__.__name__}...\")\n",
    "\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    for train_index, test_index in kfold.split(X_scaled, y_train1):\n",
    "        x_train, x_test = X_new[train_index], X_new[test_index]\n",
    "        y_train, y_test = y_train1[train_index], y_train1[test_index]\n",
    "\n",
    "        \n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        \n",
    "        y_pred = model.predict(x_test)\n",
    "\n",
    "       \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        precisions.append(precision)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        recalls.append(recall)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        f1s.append(f1)\n",
    "  \n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    mean_precision = np.mean(precisions)\n",
    "    mean_recall = np.mean(recalls)\n",
    "    mean_f1 = np.mean(f1s)\n",
    "    print(f\"Average accuracy: {mean_accuracy:.4f}\")\n",
    "    print(f\"Average precision: {mean_precision:.4f}\")\n",
    "    print(f\"Average recall: {mean_recall:.4f}\")\n",
    "    print(f\"Average f1: {mean_f1:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238dc7a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cyclopeptide]",
   "language": "python",
   "name": "conda-env-cyclopeptide-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
