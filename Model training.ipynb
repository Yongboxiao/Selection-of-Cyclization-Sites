{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80040987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1d66e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "DATA_DIR = r'D:\\data'\n",
    "df = pd.read_csv(r'D:\\data\\dataset.csv')\n",
    "pd.set_option('display.max_columns',None)\n",
    "np.set_printoptions(threshold=np.inf) \n",
    "train = {}\n",
    "train = df.to_dict()\n",
    "train1 = train['Sequence']\n",
    "train2 = train['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3e5a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "char_to_num = {\n",
    "    'A': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7,\n",
    "    'I': 8, 'K': 9, 'L': 10, 'M': 11, 'N': 12, 'P': 13,\n",
    "    'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'V': 18, 'W': 19, 'Y': 20\n",
    "}\n",
    "\n",
    "def OE1(seq_temp1):\n",
    "    fea1 = [[char_to_num.get(char, 0)] for char in seq_temp1]\n",
    "    return fea1\n",
    "\n",
    "train1_oe1 = [OE1(train1[i]) for i in train1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1977df5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(306, 20)\n",
      "[[0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.5        0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.5        0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.5        0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.5        0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.5        0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.5       ]\n",
      " [0.         0.5        0.         0.         0.5        0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.5        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.5        0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.5        0.         0.         0.         0.         0.\n",
      "  0.5        0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.5        0.         0.\n",
      "  0.         0.         0.5        0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.5        0.         0.         0.         0.\n",
      "  0.5        0.        ]\n",
      " [0.         0.5        0.         0.         0.         0.\n",
      "  0.         0.         0.5        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.5        0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.5        0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.5        0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.5       ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.5        0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.5        0.         0.\n",
      "  0.5        0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.5        0.5        0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.5        0.         0.5        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.5        0.5       ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.5        0.5        0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.5        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.5        0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.5        0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.5        0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.5        0.\n",
      "  0.         0.        ]\n",
      " [0.         0.5        0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.5        0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.5        0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.5        0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.5        0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.5        0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.5        0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.5        0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.5       ]\n",
      " [0.5        0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.5        0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.5        0.         0.5\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.5        0.         0.         0.5        0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.5        0.         0.5        0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.5        0.         0.         0.5        0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.5       ]\n",
      " [0.         0.         0.5        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.5        0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.5        0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.33333333 0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333]\n",
      " [0.         0.         0.33333333 0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.33333333 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.33333333 0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33333333 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.66666667\n",
      "  0.         0.         0.33333333 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.66666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.33333333 0.         0.\n",
      "  0.         0.33333333]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.33333333]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333 0.         0.         0.         0.\n",
      "  0.         0.33333333 0.         0.         0.         0.\n",
      "  0.33333333 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.33333333 0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.33333333 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33333333 0.33333333 0.33333333 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333 0.         0.         0.         0.\n",
      "  0.         0.33333333]\n",
      " [0.         0.33333333 0.         0.         0.         0.33333333\n",
      "  0.33333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.33333333 0.         0.33333333 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333 0.         0.         0.         0.\n",
      "  0.         0.33333333]\n",
      " [0.         0.         0.33333333 0.         0.         0.33333333\n",
      "  0.33333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.        ]\n",
      " [0.         0.         0.         0.33333333 0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33333333 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.33333333 0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.33333333 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.33333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333]\n",
      " [0.         0.         0.         0.         0.33333333 0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333 0.         0.         0.         0.\n",
      "  0.         0.33333333]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33333333 0.         0.         0.33333333\n",
      "  0.         0.        ]\n",
      " [0.33333333 0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.33333333 0.         0.         0.33333333\n",
      "  0.         0.         0.         0.33333333 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.33333333 0.33333333\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.33333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33333333 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.33333333 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.         0.         0.33333333 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.33333333 0.         0.         0.         0.         0.33333333\n",
      "  0.         0.33333333 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.33333333 0.         0.33333333\n",
      "  0.33333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.33333333 0.         0.         0.         0.\n",
      "  0.         0.33333333 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.        ]\n",
      " [0.         0.         0.         0.         0.33333333 0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.33333333 0.         0.\n",
      "  0.33333333 0.         0.         0.         0.         0.33333333\n",
      "  0.         0.        ]\n",
      " [0.         0.33333333 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.33333333]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.         0.33333333 0.33333333 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.         0.33333333 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.33333333 0.33333333 0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.33333333 0.         0.\n",
      "  0.         0.         0.33333333 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.        ]\n",
      " [0.33333333 0.         0.         0.         0.33333333 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.        ]\n",
      " [0.         0.33333333 0.         0.         0.         0.\n",
      "  0.33333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.33333333 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.66666667]\n",
      " [0.         0.33333333 0.33333333 0.         0.         0.\n",
      "  0.         0.         0.         0.33333333 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.33333333 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.33333333 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333 0.         0.         0.         0.\n",
      "  0.         0.33333333 0.         0.         0.         0.33333333\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333 0.         0.         0.33333333 0.\n",
      "  0.         0.33333333 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33333333 0.         0.         0.\n",
      "  0.         0.33333333]\n",
      " [0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.         0.33333333 0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.33333333 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.33333333]\n",
      " [0.         0.         0.         0.33333333 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33333333 0.         0.         0.33333333\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.33333333 0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.25       0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.25       0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.25       0.         0.25\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.25       0.         0.         0.25       0.         0.\n",
      "  0.         0.        ]\n",
      " [0.25       0.         0.         0.         0.         0.25\n",
      "  0.         0.         0.25       0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.25      ]\n",
      " [0.         0.         0.         0.         0.25       0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.25      ]\n",
      " [0.         0.         0.         0.         0.5        0.25\n",
      "  0.         0.         0.25       0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.25       0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.25       0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.25       0.5\n",
      "  0.         0.         0.25       0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.25       0.         0.         0.         0.         0.25\n",
      "  0.         0.         0.25       0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.25      ]\n",
      " [0.         0.         0.         0.         0.         0.25\n",
      "  0.         0.         0.         0.         0.25       0.25\n",
      "  0.25       0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.25\n",
      "  0.         0.         0.25       0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.25      ]\n",
      " [0.         0.         0.25       0.         0.         0.25\n",
      "  0.         0.25       0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25       0.25\n",
      "  0.25       0.         0.         0.25       0.         0.\n",
      "  0.         0.        ]\n",
      " [0.5        0.25       0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.25      ]\n",
      " [0.         0.         0.25       0.         0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.        ]\n",
      " [0.25       0.         0.         0.         0.         0.25\n",
      "  0.         0.         0.25       0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.25       0.25\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.25       0.         0.\n",
      "  0.         0.25      ]\n",
      " [0.5        0.         0.         0.         0.25       0.25\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.75       0.25       0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.25\n",
      "  0.         0.         0.         0.25       0.         0.\n",
      "  0.25       0.         0.         0.         0.25       0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.25       0.         0.25\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.25       0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.25       0.         0.25       0.         0.         0.\n",
      "  0.25       0.        ]\n",
      " [0.         0.         0.         0.         0.         0.25\n",
      "  0.         0.25       0.         0.         0.         0.\n",
      "  0.25       0.         0.         0.25       0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.25       0.25\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.25       0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.25       0.         0.         0.25\n",
      "  0.         0.         0.25       0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.25      ]\n",
      " [0.25       0.         0.         0.         0.25       0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.25       0.         0.         0.         0.\n",
      "  0.         0.25      ]\n",
      " [0.         0.         0.         0.         0.25       0.25\n",
      "  0.         0.25       0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.        ]\n",
      " [0.25       0.         0.5        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.25       0.         0.25       0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.25      ]\n",
      " [0.25       0.         0.75       0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.25       0.25\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.25       0.         0.         0.         0.25       0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.25       0.         0.25       0.         0.\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.        ]\n",
      " [0.         0.25       0.         0.         0.25       0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.25       0.         0.         0.         0.         0.25\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.25       0.         0.         0.\n",
      "  0.         0.25      ]\n",
      " [0.         0.         0.25       0.         0.         0.25\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.25       0.         0.         0.         0.25       0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.25       0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.25       0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.25\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.25       0.         0.         0.25       0.         0.\n",
      "  0.         0.        ]\n",
      " [0.25       0.25       0.         0.         0.         0.25\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.25       0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.2        0.         0.4        0.         0.         0.2\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.2        0.2\n",
      "  0.         0.         0.         0.         0.2        0.2\n",
      "  0.         0.         0.         0.2        0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.2        0.         0.2\n",
      "  0.         0.         0.         0.         0.2        0.2\n",
      "  0.         0.         0.         0.2        0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.2        0.         0.         0.2        0.4\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.2       ]\n",
      " [0.2        0.         0.         0.         0.2        0.2\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.2        0.         0.         0.\n",
      "  0.         0.2       ]\n",
      " [0.         0.         0.         0.         0.         0.2\n",
      "  0.         0.         0.8        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.2        0.2        0.2\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.2        0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.2        0.\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.2        0.         0.2        0.         0.         0.\n",
      "  0.2        0.        ]\n",
      " [0.         0.         0.         0.         0.         0.4\n",
      "  0.         0.2        0.         0.         0.         0.\n",
      "  0.2        0.         0.         0.2        0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.2        0.         0.         0.2\n",
      "  0.         0.         0.2        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.         0.2       ]\n",
      " [0.4        0.         0.         0.         0.2        0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.2        0.         0.         0.         0.\n",
      "  0.         0.2       ]\n",
      " [0.         0.         0.2        0.         0.         0.2\n",
      "  0.         0.         0.2        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.         0.2       ]\n",
      " [0.         0.         0.2        0.         0.         0.2\n",
      "  0.         0.2        0.         0.2        0.         0.\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.2\n",
      "  0.         0.         0.         0.         0.2        0.2\n",
      "  0.2        0.         0.         0.2        0.         0.\n",
      "  0.         0.        ]\n",
      " [0.4        0.2        0.         0.         0.2        0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.2       ]\n",
      " [0.2        0.         0.4        0.         0.         0.2\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.2\n",
      "  0.2        0.         0.2        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.         0.2       ]\n",
      " [0.2        0.         0.2        0.         0.         0.2\n",
      "  0.         0.         0.2        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.         0.        ]\n",
      " [0.2        0.         0.6        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.2        0.         0.2        0.2\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.2        0.         0.         0.         0.2        0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.2        0.2\n",
      "  0.         0.2        0.         0.2        0.         0.\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.         0.        ]\n",
      " [0.4        0.         0.         0.         0.2        0.2\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.2        0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.6        0.2        0.         0.         0.2        0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.2        0.2\n",
      "  0.         0.         0.         0.2        0.         0.\n",
      "  0.2        0.         0.         0.         0.2        0.\n",
      "  0.         0.        ]\n",
      " [0.2        0.         0.2        0.         0.         0.2\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2        0.2\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.2        0.2        0.2\n",
      "  0.         0.         0.4        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.2        0.         0.2\n",
      "  0.         0.         0.         0.         0.2        0.2\n",
      "  0.2        0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.2        0.2\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.2        0.         0.         0.2        0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.2        0.2\n",
      "  0.         0.2        0.         0.         0.         0.\n",
      "  0.2        0.         0.         0.         0.2        0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.2        0.2\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.2        0.         0.         0.2        0.         0.\n",
      "  0.         0.2       ]\n",
      " [0.         0.         0.2        0.         0.         0.4\n",
      "  0.         0.         0.2        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.2        0.         0.         0.2\n",
      "  0.         0.         0.         0.2        0.         0.\n",
      "  0.2        0.         0.         0.         0.2        0.\n",
      "  0.         0.        ]\n",
      " [0.2        0.2        0.         0.         0.         0.2\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.2        0.         0.         0.         0.\n",
      "  0.         0.2       ]\n",
      " [0.4        0.         0.         0.         0.         0.4\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.2        0.        ]\n",
      " [0.         0.         0.         0.         0.16666667 0.33333333\n",
      "  0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.16666667 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.16666667 0.         0.16666667 0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.16666667 0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.14285714 0.         0.28571429 0.28571429\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.14285714 0.\n",
      "  0.         0.14285714]\n",
      " [0.         0.         0.16666667 0.         0.5        0.16666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.16666667 0.         0.         0.16666667\n",
      "  0.         0.16666667 0.         0.16666667 0.         0.\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.        ]\n",
      " [0.33333333 0.         0.         0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.16666667 0.         0.         0.         0.\n",
      "  0.         0.16666667]\n",
      " [0.16666667 0.16666667 0.         0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.16666667]\n",
      " [0.         0.         0.16666667 0.         0.16666667 0.33333333\n",
      "  0.16666667 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.16666667 0.         0.5        0.         0.         0.16666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.16666667 0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.16666667 0.         0.16666667 0.\n",
      "  0.         0.16666667]\n",
      " [0.         0.         0.16666667 0.         0.         0.33333333\n",
      "  0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.16666667]\n",
      " [0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.16666667 0.         0.         0.16666667 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.16666667 0.33333333\n",
      "  0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.16666667 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.16666667 0.         0.         0.16666667 0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.16666667]\n",
      " [0.         0.         0.16666667 0.         0.         0.16666667\n",
      "  0.16666667 0.         0.16666667 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.16666667]\n",
      " [0.         0.         0.16666667 0.         0.16666667 0.33333333\n",
      "  0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.16666667 0.         0.16666667 0.         0.         0.33333333\n",
      "  0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.5        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.33333333 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.16666667 0.         0.16666667 0.16666667\n",
      "  0.         0.16666667 0.         0.16666667 0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.16666667\n",
      "  0.         0.         0.83333333 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.16666667 0.         0.33333333 0.         0.         0.16666667\n",
      "  0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.16666667 0.16666667 0.16666667\n",
      "  0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.16666667 0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.16666667 0.16666667 0.16666667\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.16666667 0.         0.         0.16666667 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.16666667 0.33333333\n",
      "  0.         0.16666667 0.         0.         0.         0.\n",
      "  0.16666667 0.         0.         0.16666667 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.16666667 0.         0.         0.33333333\n",
      "  0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.16666667]\n",
      " [0.         0.         0.16666667 0.         0.         0.16666667\n",
      "  0.         0.16666667 0.         0.16666667 0.         0.\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.16666667 0.         0.16666667\n",
      "  0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.16666667 0.         0.         0.16666667 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.33333333 0.16666667 0.         0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.16666667]\n",
      " [0.16666667 0.         0.33333333 0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.16666667 0.         0.         0.16666667\n",
      "  0.16666667 0.         0.16666667 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.16666667]\n",
      " [0.16666667 0.         0.33333333 0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.16666667 0.         0.16666667 0.         0.         0.16666667\n",
      "  0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.         0.        ]\n",
      " [0.16666667 0.         0.5        0.         0.         0.16666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.16666667 0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.16666667 0.         0.\n",
      "  0.16666667 0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.16666667 0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.16666667 0.         0.         0.16666667 0.         0.\n",
      "  0.         0.16666667]\n",
      " [0.33333333 0.         0.         0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.16666667]\n",
      " [0.5        0.16666667 0.         0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.16666667 0.         0.16666667 0.         0.         0.16666667\n",
      "  0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.         0.16666667 0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.16666667 0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.16666667 0.         0.\n",
      "  0.16666667 0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.16666667 0.         0.         0.         0.16666667 0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.16666667 0.         0.16666667 0.         0.         0.\n",
      "  0.16666667 0.        ]\n",
      " [0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.16666667 0.         0.         0.16666667 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.16666667 0.         0.         0.33333333\n",
      "  0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.16666667]\n",
      " [0.33333333 0.         0.         0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.16666667 0.         0.         0.         0.\n",
      "  0.         0.16666667]\n",
      " [0.         0.         0.16666667 0.         0.16666667 0.16666667\n",
      "  0.         0.16666667 0.         0.         0.         0.\n",
      "  0.16666667 0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.16666667 0.         0.\n",
      "  0.33333333 0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.         0.16666667 0.         0.16666667 0.         0.\n",
      "  0.16666667 0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.         0.         0.         0.16666667 0.         0.\n",
      "  0.33333333 0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.14285714 0.14285714 0.14285714\n",
      "  0.         0.         0.         0.         0.14285714 0.14285714\n",
      "  0.14285714 0.         0.         0.14285714 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.14285714 0.         0.42857143 0.         0.         0.28571429\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.14285714 0.\n",
      "  0.         0.        ]\n",
      " [0.28571429 0.         0.14285714 0.         0.         0.14285714\n",
      "  0.         0.         0.14285714 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.14285714 0.14285714\n",
      "  0.         0.        ]\n",
      " [0.28571429 0.14285714 0.         0.         0.14285714 0.14285714\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.14285714 0.         0.         0.\n",
      "  0.         0.14285714]\n",
      " [0.         0.         0.         0.         0.         0.28571429\n",
      "  0.         0.28571429 0.         0.         0.         0.\n",
      "  0.28571429 0.         0.         0.         0.14285714 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.14285714 0.14285714\n",
      "  0.         0.         0.         0.14285714 0.         0.\n",
      "  0.28571429 0.         0.         0.         0.14285714 0.\n",
      "  0.14285714 0.        ]\n",
      " [0.         0.         0.         0.         0.14285714 0.14285714\n",
      "  0.         0.28571429 0.         0.         0.         0.\n",
      "  0.14285714 0.         0.         0.         0.14285714 0.14285714\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.14285714 0.28571429\n",
      "  0.         0.14285714 0.         0.         0.         0.\n",
      "  0.14285714 0.         0.         0.14285714 0.         0.\n",
      "  0.         0.14285714]\n",
      " [0.         0.         0.         0.14285714 0.14285714 0.14285714\n",
      "  0.         0.         0.         0.         0.28571429 0.\n",
      "  0.14285714 0.         0.         0.14285714 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.14285714 0.         0.         0.         0.14285714 0.\n",
      "  0.         0.         0.         0.         0.14285714 0.\n",
      "  0.14285714 0.14285714 0.14285714 0.         0.         0.\n",
      "  0.14285714 0.        ]\n",
      " [0.         0.         0.         0.         0.14285714 0.28571429\n",
      "  0.         0.14285714 0.         0.         0.         0.\n",
      "  0.14285714 0.         0.         0.14285714 0.         0.\n",
      "  0.         0.14285714]\n",
      " [0.         0.         0.         0.         0.14285714 0.14285714\n",
      "  0.         0.         0.         0.         0.14285714 0.14285714\n",
      "  0.14285714 0.         0.         0.14285714 0.         0.\n",
      "  0.         0.14285714]\n",
      " [0.         0.         0.14285714 0.         0.         0.28571429\n",
      "  0.         0.         0.14285714 0.         0.         0.14285714\n",
      "  0.         0.         0.         0.         0.14285714 0.\n",
      "  0.         0.14285714]\n",
      " [0.         0.14285714 0.14285714 0.         0.         0.28571429\n",
      "  0.         0.         0.14285714 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.14285714 0.\n",
      "  0.         0.14285714]\n",
      " [0.         0.         0.14285714 0.         0.         0.14285714\n",
      "  0.14285714 0.14285714 0.         0.14285714 0.         0.\n",
      "  0.         0.         0.         0.         0.28571429 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.14285714 0.14285714 0.14285714\n",
      "  0.         0.         0.         0.         0.14285714 0.14285714\n",
      "  0.14285714 0.         0.         0.14285714 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.28571429 0.14285714 0.         0.         0.14285714 0.14285714\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.14285714 0.         0.         0.\n",
      "  0.         0.14285714]\n",
      " [0.14285714 0.         0.42857143 0.         0.         0.28571429\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.14285714 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.14285714 0.14285714 0.         0.14285714\n",
      "  0.14285714 0.         0.14285714 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.14285714 0.\n",
      "  0.         0.14285714]\n",
      " [0.14285714 0.         0.42857143 0.         0.         0.28571429\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.14285714 0.\n",
      "  0.         0.        ]\n",
      " [0.28571429 0.         0.14285714 0.         0.         0.14285714\n",
      "  0.         0.         0.14285714 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.14285714 0.14285714\n",
      "  0.         0.        ]\n",
      " [0.14285714 0.         0.42857143 0.         0.         0.28571429\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.14285714 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.14285714 0.         0.14285714 0.14285714\n",
      "  0.         0.14285714 0.         0.14285714 0.         0.\n",
      "  0.14285714 0.         0.         0.         0.14285714 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.14285714 0.         0.14285714 0.14285714\n",
      "  0.         0.14285714 0.         0.14285714 0.         0.\n",
      "  0.14285714 0.         0.         0.         0.14285714 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.14285714 0.28571429\n",
      "  0.         0.14285714 0.         0.         0.         0.\n",
      "  0.14285714 0.         0.         0.14285714 0.         0.\n",
      "  0.         0.14285714]\n",
      " [0.42857143 0.         0.         0.         0.14285714 0.14285714\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.14285714 0.         0.         0.\n",
      "  0.         0.14285714]\n",
      " [0.42857143 0.14285714 0.         0.         0.14285714 0.14285714\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.14285714 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.14285714 0.14285714\n",
      "  0.         0.         0.         0.14285714 0.         0.\n",
      "  0.28571429 0.         0.         0.         0.14285714 0.\n",
      "  0.14285714 0.        ]\n",
      " [0.         0.         0.14285714 0.         0.14285714 0.14285714\n",
      "  0.         0.14285714 0.         0.14285714 0.         0.\n",
      "  0.14285714 0.         0.         0.         0.14285714 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.14285714 0.14285714 0.14285714\n",
      "  0.         0.         0.         0.         0.14285714 0.14285714\n",
      "  0.14285714 0.         0.         0.14285714 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.28571429 0.         0.14285714 0.         0.14285714 0.14285714\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.14285714 0.         0.         0.         0.\n",
      "  0.         0.14285714]\n",
      " [0.         0.         0.14285714 0.         0.14285714 0.14285714\n",
      "  0.         0.14285714 0.         0.14285714 0.         0.\n",
      "  0.14285714 0.         0.         0.         0.14285714 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.14285714\n",
      "  0.         0.28571429 0.         0.         0.         0.\n",
      "  0.28571429 0.         0.         0.         0.14285714 0.14285714\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.28571429\n",
      "  0.         0.28571429 0.         0.         0.         0.\n",
      "  0.14285714 0.         0.         0.         0.14285714 0.14285714\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.14285714\n",
      "  0.         0.28571429 0.         0.         0.         0.\n",
      "  0.28571429 0.         0.         0.         0.14285714 0.14285714\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.14285714 0.14285714 0.14285714\n",
      "  0.         0.         0.         0.         0.14285714 0.14285714\n",
      "  0.14285714 0.         0.         0.14285714 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.14285714 0.14285714 0.         0.         0.14285714 0.14285714\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.14285714 0.14285714 0.         0.         0.\n",
      "  0.         0.14285714]\n",
      " [0.14285714 0.14285714 0.         0.         0.14285714 0.14285714\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.14285714 0.14285714 0.         0.         0.\n",
      "  0.         0.14285714]\n",
      " [0.         0.         0.         0.125      0.125      0.25\n",
      "  0.         0.         0.         0.         0.125      0.125\n",
      "  0.125      0.         0.         0.125      0.         0.\n",
      "  0.         0.        ]\n",
      " [0.375      0.125      0.         0.         0.125      0.125\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.125      0.         0.         0.\n",
      "  0.         0.125     ]\n",
      " [0.         0.         0.         0.         0.125      0.25\n",
      "  0.         0.125      0.         0.125      0.         0.\n",
      "  0.375      0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.25       0.         0.375      0.         0.         0.25\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.125      0.\n",
      "  0.         0.        ]\n",
      " [0.125      0.         0.375      0.         0.         0.25\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.125      0.         0.125      0.\n",
      "  0.         0.        ]\n",
      " [0.125      0.         0.375      0.         0.         0.25\n",
      "  0.125      0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.125      0.\n",
      "  0.         0.        ]\n",
      " [0.25       0.         0.125      0.125      0.         0.125\n",
      "  0.         0.         0.125      0.         0.         0.\n",
      "  0.         0.         0.         0.         0.125      0.125\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.125      0.125\n",
      "  0.         0.         0.125      0.125      0.         0.\n",
      "  0.25       0.         0.         0.         0.125      0.\n",
      "  0.125      0.        ]\n",
      " [0.125      0.         0.375      0.         0.         0.25\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.125      0.125      0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.125      0.         0.125      0.125\n",
      "  0.         0.125      0.         0.125      0.125      0.\n",
      "  0.125      0.         0.         0.         0.125      0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.125      0.         0.125      0.125\n",
      "  0.         0.125      0.         0.125      0.         0.\n",
      "  0.125      0.         0.125      0.         0.125      0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.125      0.25\n",
      "  0.         0.125      0.         0.         0.         0.\n",
      "  0.125      0.         0.         0.125      0.         0.125\n",
      "  0.         0.125     ]\n",
      " [0.375      0.125      0.         0.         0.125      0.125\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.125      0.         0.         0.\n",
      "  0.         0.125     ]\n",
      " [0.375      0.         0.375      0.         0.         0.125\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.125      0.\n",
      "  0.         0.        ]\n",
      " [0.125      0.125      0.         0.         0.125      0.125\n",
      "  0.         0.         0.125      0.         0.         0.\n",
      "  0.         0.125      0.125      0.         0.         0.\n",
      "  0.         0.125     ]\n",
      " [0.25       0.125      0.         0.         0.125      0.125\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.125      0.125      0.         0.         0.\n",
      "  0.         0.125     ]\n",
      " [0.375      0.125      0.         0.         0.125      0.125\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.125      0.         0.         0.\n",
      "  0.         0.125     ]\n",
      " [0.         0.125      0.         0.         0.125      0.125\n",
      "  0.         0.         0.         0.125      0.         0.\n",
      "  0.25       0.         0.         0.         0.125      0.\n",
      "  0.125      0.        ]\n",
      " [0.22222222 0.         0.33333333 0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.11111111 0.\n",
      "  0.         0.        ]\n",
      " [0.44444444 0.         0.33333333 0.         0.         0.11111111\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.11111111 0.\n",
      "  0.         0.        ]\n",
      " [0.11111111 0.         0.33333333 0.         0.         0.22222222\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.11111111 0.         0.11111111 0.\n",
      "  0.11111111 0.        ]\n",
      " [0.11111111 0.         0.33333333 0.         0.         0.22222222\n",
      "  0.11111111 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.11111111 0.         0.11111111 0.\n",
      "  0.         0.        ]\n",
      " [0.22222222 0.         0.11111111 0.11111111 0.         0.11111111\n",
      "  0.         0.         0.11111111 0.         0.         0.\n",
      "  0.         0.         0.         0.11111111 0.11111111 0.11111111\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.11111111 0.11111111\n",
      "  0.         0.         0.11111111 0.11111111 0.11111111 0.\n",
      "  0.22222222 0.         0.         0.         0.11111111 0.\n",
      "  0.11111111 0.        ]\n",
      " [0.11111111 0.         0.33333333 0.         0.         0.22222222\n",
      "  0.         0.         0.         0.         0.         0.11111111\n",
      "  0.         0.         0.         0.11111111 0.11111111 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.11111111 0.22222222\n",
      "  0.11111111 0.11111111 0.         0.         0.         0.\n",
      "  0.11111111 0.         0.         0.11111111 0.         0.11111111\n",
      "  0.         0.11111111]\n",
      " [0.33333333 0.11111111 0.         0.         0.11111111 0.11111111\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.11111111 0.         0.11111111 0.\n",
      "  0.         0.11111111]\n",
      " [0.         0.         0.11111111 0.11111111 0.         0.11111111\n",
      "  0.11111111 0.         0.11111111 0.         0.         0.\n",
      "  0.         0.11111111 0.         0.         0.11111111 0.11111111\n",
      "  0.         0.11111111]\n",
      " [0.         0.11111111 0.11111111 0.         0.11111111 0.11111111\n",
      "  0.         0.11111111 0.         0.11111111 0.11111111 0.\n",
      "  0.11111111 0.         0.         0.         0.11111111 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.11111111 0.         0.11111111 0.11111111\n",
      "  0.         0.11111111 0.11111111 0.11111111 0.         0.\n",
      "  0.11111111 0.         0.11111111 0.         0.11111111 0.\n",
      "  0.         0.        ]\n",
      " [0.33333333 0.11111111 0.         0.         0.11111111 0.11111111\n",
      "  0.11111111 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.11111111 0.         0.         0.\n",
      "  0.         0.11111111]\n",
      " [0.2        0.         0.3        0.         0.1        0.3\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.1        0.\n",
      "  0.         0.        ]\n",
      " [0.1        0.         0.2        0.         0.         0.5\n",
      "  0.         0.         0.1        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.1        0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.1        0.1        0.4\n",
      "  0.         0.         0.         0.         0.1        0.1\n",
      "  0.1        0.         0.         0.1        0.         0.\n",
      "  0.         0.        ]\n",
      " [0.6        0.         0.         0.         0.1        0.1\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.1        0.         0.         0.\n",
      "  0.         0.1       ]\n",
      " [0.09090909 0.         0.18181818 0.         0.         0.54545455\n",
      "  0.         0.         0.09090909 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.09090909 0.\n",
      "  0.         0.        ]\n",
      " [0.36363636 0.         0.27272727 0.         0.         0.27272727\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.09090909 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.08333333 0.08333333 0.41666667\n",
      "  0.         0.         0.         0.         0.16666667 0.08333333\n",
      "  0.08333333 0.         0.         0.08333333 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.08333333 0.08333333 0.5\n",
      "  0.         0.         0.         0.         0.08333333 0.08333333\n",
      "  0.08333333 0.         0.         0.08333333 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.25       0.         0.         0.         0.25       0.25\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.25       0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.58333333 0.08333333 0.         0.         0.08333333 0.08333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.08333333 0.         0.         0.         0.\n",
      "  0.         0.08333333]\n",
      " [0.         0.         0.25       0.         0.08333333 0.33333333\n",
      "  0.         0.         0.08333333 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.16666667 0.         0.16666667 0.16666667\n",
      "  0.         0.16666667 0.         0.16666667 0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.        ]\n",
      " [0.5        0.08333333 0.         0.         0.08333333 0.08333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.08333333 0.08333333 0.         0.         0.\n",
      "  0.         0.08333333]\n",
      " [0.         0.         0.         0.07692308 0.07692308 0.46153846\n",
      "  0.         0.         0.         0.         0.15384615 0.07692308\n",
      "  0.07692308 0.         0.         0.07692308 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.30769231 0.         0.23076923 0.         0.         0.38461538\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.07692308 0.\n",
      "  0.         0.        ]\n",
      " [0.25       0.         0.25       0.         0.         0.25\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25       0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.23076923 0.         0.07692308 0.23076923\n",
      "  0.         0.07692308 0.         0.15384615 0.         0.\n",
      "  0.07692308 0.         0.         0.         0.15384615 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.07142857 0.07142857 0.5\n",
      "  0.         0.         0.         0.         0.14285714 0.07142857\n",
      "  0.07142857 0.         0.         0.07142857 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.21428571 0.         0.07142857 0.42857143\n",
      "  0.         0.         0.07142857 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.21428571 0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Amino acid composition (AAC)\n",
    "handcraft_AAC_test = [[0] * 20 for _ in range(len(train1_oe1))]\n",
    "for row in range(len(train1_oe1)):\n",
    "    seq = train1_oe1[row]\n",
    "    for i in seq:\n",
    "        col = i[0]-1\n",
    "        handcraft_AAC_test[row][col] += 1/len(seq)\n",
    "hc_AAC_test = np.array(handcraft_AAC_test)\n",
    "print(hc_AAC_test.shape)\n",
    "print(hc_AAC_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e99d8ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(306, 400)\n",
      "Length of feature vector: 400\n"
     ]
    }
   ],
   "source": [
    "#Dipeptide composition (DPC)\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def compute_dpc_pairs(sequence, k):\n",
    "    return [sequence[i] + sequence[i + k + 1] for i in range(len(sequence) - k - 1)]\n",
    "\n",
    "def calculate_amino_acid_pairs_frequency(sequence, max_k):\n",
    "    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "    pair_count = len(amino_acids) ** 2\n",
    "    feature_vector = []\n",
    "\n",
    "    for k in range(max_k + 1):\n",
    "        dpc_pairs = compute_dpc_pairs(sequence, k)\n",
    "        pair_counter = Counter(dpc_pairs)\n",
    "        total_pairs = len(dpc_pairs)\n",
    "\n",
    "        vector = [pair_counter.get(a + b, 0) / total_pairs for a in amino_acids for b in amino_acids]\n",
    "        feature_vector.extend(vector)\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "max_k = 0\n",
    "dpc_group_pairs = [calculate_amino_acid_pairs_frequency(sequence, max_k) for sequence in train1.values()]\n",
    "DPC = np.array(dpc_group_pairs)\n",
    "\n",
    "print(DPC.shape)\n",
    "print(\"Length of feature vector:\", len(dpc_group_pairs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b6ff86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(306, 280)\n"
     ]
    }
   ],
   "source": [
    "#The One-Hot descriptor for sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "amino_index = {aa: i for i, aa in enumerate(amino_acids)}\n",
    "max_length = max(df['Sequence'].apply(len))\n",
    "\n",
    "def sequence_to_one_hot(seq):\n",
    "    one_hot = np.zeros(max_length*20)\n",
    "    \n",
    "    for i, aa in enumerate(seq):\n",
    "        if i >= 14:\n",
    "            break\n",
    "        if aa in amino_index:\n",
    "            index = amino_index[aa] + i * 20\n",
    "            one_hot[index] = 1\n",
    "            \n",
    "    return one_hot\n",
    "\n",
    "sequences = df['Sequence']\n",
    "\n",
    "one_hot_encoded = np.array([sequence_to_one_hot(seq) for seq in sequences])\n",
    "\n",
    "print(one_hot_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d057ce4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated featuresPC: [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 0], [0, 0], [0, 0], [0, 1], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 0], [1, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 0], [1, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 0], [1, 0], [1, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 1], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
      "Generated featuresF: [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 1, 1], [0, 0, 1, 1], [0, 0, 1, 1], [0, 0, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 1, 1], [0, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [1, 0, 1, 1], [0, 0, 0, 0], [0, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [1, 0, 1, 1], [1, 0, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
      "Generated featuresM: [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [1, 0, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [1, 0, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [1, 0, 1], [0, 0, 0], [0, 0, 0], [1, 0, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [1, 0, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [1, 0, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [1, 1, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [1, 1, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [1, 1, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [1, 1, 1], [1, 1, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [1, 1, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [1, 1, 1], [0, 0, 0]]\n",
      "Generated featuresT: [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 1], [1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 1, 1], [1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 1], [1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 1], [0, 0, 0, 0, 1, 1], [1, 0, 0, 0, 1, 1], [1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 1], [1, 0, 0, 0, 1, 1], [1, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 1], [0, 0, 1, 0, 0, 1], [0, 0, 1, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 1, 1], [0, 0, 1, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 1, 1], [1, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 1, 1], [1, 1, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1], [1, 1, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 1], [1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 1, 1], [1, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 1, 1]]\n",
      "Generated featuresG: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "(306, 16)\n"
     ]
    }
   ],
   "source": [
    "#Hand-crafted features\n",
    "def summarized_featuresPC(peptide_sequences):\n",
    "    summarized_featuresPC = []\n",
    "    for i in peptide_sequences:\n",
    "        last_aa = peptide_sequences[i][0]  # Get the last amino acid\n",
    "        if last_aa == 'P':\n",
    "            summarized_featuresPC.append([1,0])  #If the last amino acid is Pro\n",
    "        elif last_aa == 'C':\n",
    "            summarized_featuresPC.append([0,1])\n",
    "        else:\n",
    "            summarized_featuresPC.append([0,0])\n",
    "    return summarized_featuresPC\n",
    "def summarized_featuresF(peptide_sequences):\n",
    "    summarized_featuresF = []\n",
    "    for i in peptide_sequences:\n",
    "        last_aa = peptide_sequences[i][0]\n",
    "        first_aa = peptide_sequences[i][-1]\n",
    "        first3_aa = peptide_sequences[i][-3:]\n",
    "        if last_aa == 'F':\n",
    "            if first3_aa == 'CRG':\n",
    "                summarized_featuresF.append([1,0,1,1])\n",
    "            elif first3_aa == 'YRG':\n",
    "                summarized_featuresF.append([0,1,1,1])\n",
    "            elif first_aa == 'G':\n",
    "                summarized_featuresF.append([0,0,1,1])\n",
    "            else:\n",
    "                summarized_featuresF.append([0,0,0,1])\n",
    "        else:\n",
    "            summarized_featuresF.append([0,0,0,0])\n",
    "    return summarized_featuresF\n",
    "def summarized_featuresM(peptide_sequences):\n",
    "    summarized_featuresM = []\n",
    "    for i in peptide_sequences:\n",
    "        last_aa = peptide_sequences[i][0]\n",
    "        first_aa = peptide_sequences[i][-1]\n",
    "        first6_aa = peptide_sequences[i][-6:]\n",
    "        if last_aa == 'M':\n",
    "            if first6_aa == 'PNSFEG':\n",
    "                summarized_featuresM.append([1,1,1])\n",
    "            elif first_aa == 'G':\n",
    "                summarized_featuresM.append([1,0,1])\n",
    "            else:\n",
    "                summarized_featuresM.append([0,0,1])\n",
    "        else:\n",
    "            summarized_featuresM.append([0,0,0])\n",
    "    return summarized_featuresM\n",
    "def summarized_featuresT(peptide_sequences):\n",
    "    summarized_featuresT = []\n",
    "    for i in peptide_sequences:\n",
    "        last_aa = peptide_sequences[i][0]\n",
    "        last2_aa = peptide_sequences[i][0:2]\n",
    "        first_aa = peptide_sequences[i][-1]\n",
    "        first2_aa = peptide_sequences[i][-2:]\n",
    "        if last_aa == 'T':\n",
    "            if last2_aa == 'TD':\n",
    "                if first2_aa == 'GG':\n",
    "                    summarized_featuresT.append([1,1,0,0,1,1])\n",
    "                elif first_aa == 'G':\n",
    "                    summarized_featuresT.append([1,0,0,0,1,1])\n",
    "                else:\n",
    "                    summarized_featuresT.append([0,0,0,0,1,1])\n",
    "            elif first2_aa == 'DG':\n",
    "                summarized_featuresT.append([0,0,1,0,0,1])\n",
    "            elif first2_aa == 'FG':\n",
    "                summarized_featuresT.append([0,0,0,1,0,1])\n",
    "            elif first_aa == 'G':\n",
    "                summarized_featuresT.append([1,0,0,0,0,1])\n",
    "            else:\n",
    "                summarized_featuresT.append([0,0,0,0,0,1])\n",
    "        else:\n",
    "            summarized_featuresT.append([0,0,0,0,0,0])\n",
    "    return summarized_featuresT\n",
    "def summarized_featuresG(peptide_sequences):\n",
    "    summarized_featuresG = []\n",
    "    for i in peptide_sequences:\n",
    "        last_aa = peptide_sequences[i][0]\n",
    "        if last_aa == 'G':\n",
    "            summarized_featuresG.append(1)\n",
    "        else:\n",
    "            summarized_featuresG.append(0)\n",
    "    return summarized_featuresG\n",
    "\n",
    "\n",
    "featuresPC = summarized_featuresPC(train1)\n",
    "featuresF = summarized_featuresF(train1)\n",
    "featuresM = summarized_featuresM(train1)\n",
    "featuresT = summarized_featuresT(train1)\n",
    "featuresG = summarized_featuresG(train1)\n",
    "\n",
    "\n",
    "print(\"Generated featuresPC:\", featuresPC)\n",
    "print(\"Generated featuresF:\", featuresF)\n",
    "print(\"Generated featuresM:\", featuresM)\n",
    "print(\"Generated featuresT:\", featuresT)\n",
    "print(\"Generated featuresG:\", featuresG)\n",
    "Generated_features = np.c_[featuresPC,featuresF,featuresM,featuresT,featuresG]\n",
    "print(Generated_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff16449f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove: [12, 39, 42, 45, 47, 48, 50, 51, 52, 54, 57, 58, 59, 60, 62, 63, 67, 68, 69, 71, 72, 73, 79, 83, 84, 86, 87, 89, 90, 91, 93, 94, 97, 99, 103, 104, 105, 107, 109, 113, 114, 115, 118, 122, 126, 127, 131, 133, 135, 137, 139, 147, 151, 153, 154, 156, 159, 162, 163, 165, 166, 167, 168, 169, 171, 174, 176, 179, 182, 183, 184, 186, 187, 190, 191, 193, 194, 195, 202, 203, 205, 206, 207, 208, 209, 211, 215, 216, 217, 219, 220, 222, 223, 224, 225, 226, 227, 229, 231, 232, 233, 235, 237, 238, 239, 240, 242, 246, 249, 252, 253, 254, 256, 257, 258, 259, 262, 265, 267, 268, 269, 270, 272, 273, 274, 276, 277, 279, 284, 285, 289, 290, 294, 295, 297, 298, 299, 300, 302, 304, 305, 307, 309, 310, 311, 312, 316, 319, 320, 323, 325, 326, 327, 329, 330, 332, 334, 335, 336, 337, 338, 342, 343, 344, 345, 349, 350, 351, 354, 359, 366, 367, 369, 371, 372, 373, 374, 376, 377, 382, 384, 385, 387, 388, 390, 391, 392, 393, 395, 397, 398, 399, 400, 402, 404, 405, 406, 409, 411, 412, 413, 414, 418, 419, 423, 426, 428, 431, 434, 519, 542, 549, 553, 573, 579, 582, 590, 591, 593, 594, 597, 602, 603, 606, 609, 613, 614, 616, 619, 622, 623, 624, 625, 626, 628, 629, 633, 634, 635, 642, 643, 644, 646, 647, 648, 649, 650, 653, 654, 655, 656, 657, 662, 663, 664, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 680, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['scaler307.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "X_train = np.c_[Generated_features, hc_AAC_test, DPC, one_hot_encoded]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler = scaler.fit(X_train)\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "df2 = pd.DataFrame(X_scaled)\n",
    "df2_non_zero = df2.loc[:, (df2 != 0).any(axis=0)]\n",
    "removed_indices = df2.columns[~df2.columns.isin(df2_non_zero.columns)].tolist()\n",
    "print(\"Remove:\", removed_indices)\n",
    "X_filtered=np.array(df2_non_zero)\n",
    "joblib.dump(scaler, \"scaler307.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56537282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
      "174\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "y_train1=[]\n",
    "count1=0\n",
    "count0=0\n",
    "for idx in df.index:\n",
    "    y_train1 += [df['Score'].loc[idx]] \n",
    "    if df['Score'].loc[idx]==1:\n",
    "        count1+=1\n",
    "    else:\n",
    "        count0+=1\n",
    "y_train1 = np.array(y_train1, dtype = float)\n",
    "print(y_train1)\n",
    "print(count1)\n",
    "print(count0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e16c2e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[243  39  84   1 134 217 300 273 276 185  73 262 122   5  46 174  36 158\n",
      "  52 152 210 133 101 108   7  83 291 129 274 189 303  82 279  59  29  34\n",
      " 164 252  43 254 192 136  51 200 183 227  20  15 261  23  25 105 205 302\n",
      " 176 194 266 203  21 209 301 223 201 110  30 155 159 116 123 222 127 187\n",
      "   8  67 139  41 290  79  85  44  93  71 268 154  10  90 204 295  69 141\n",
      "  50 214 229 118 168 135  48 115 153  72 126 172  57 149 144   9  98  13\n",
      " 207 211 246  74 213 267 245 240 146 182  64 226  53 173 143 293  33  75\n",
      "   6 286 230 269  99  11  63  91 255  22 191  68  76  96 177  94 128  95\n",
      " 239 150 138 218 145 250 297 117  16 280 188 119 169 249 131 281  24 219\n",
      "  47 196 206 113 231 179  28  80 142 260 271 170 285 277 140  65  62 130\n",
      "  60 107 292  78   3 208 258  17 157 106 298 299 181  42 114  32 263  27\n",
      "  49 264 175 242 287 237  45 221 294 278 289 100  26  35 224 120]\n",
      "[ 12  66 256 184 235   0 163 272 166 186 199 193 156 296  31 284  87 241\n",
      " 167 102   4 215 178 234 305 282 304 202 165 228 162  97 232 171  77  58\n",
      " 220 132 251 283 197 125 104 160 244 236 257  18 112  19 238 225  81  55\n",
      " 248 161   2 198 212 103  92 111 265 147 247 151  40 275  14 233 180 216\n",
      " 137  61 109  38 121 124 148 259  56 253  37 190 270  86  54 195  89  70\n",
      " 288  88]\n",
      "122\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "for train_index, test_index in sss.split(X_filtered, y_train1):\n",
    "    x_train, x_test = X_filtered[train_index], X_filtered[test_index]\n",
    "    y_train, y_test = y_train1[train_index], y_train1[test_index]\n",
    "print(train_index)\n",
    "print(test_index)\n",
    "train1=0\n",
    "train0=0\n",
    "for i in y_train:\n",
    "    if i==1:\n",
    "        train1+=1\n",
    "    else:\n",
    "        train0+=1\n",
    "print(train1)\n",
    "print(train0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6e70009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0.\n",
      " 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0.\n",
      " 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
      "[1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
      "Accuracy： 0.6304347826086957\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.70      0.62        40\n",
      "         1.0       0.71      0.58      0.64        52\n",
      "\n",
      "    accuracy                           0.63        92\n",
      "   macro avg       0.64      0.64      0.63        92\n",
      "weighted avg       0.65      0.63      0.63        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GaussianNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb_classifier = GaussianNB()\n",
    "\n",
    "nb_classifier.fit(x_train, y_train)\n",
    "\n",
    "y_test_NB = nb_classifier.predict(x_test)\n",
    "print(y_test_NB)\n",
    "print(y_test)\n",
    "\n",
    "accuracy_NB = accuracy_score(y_test, y_test_NB)\n",
    "print(\"Accuracy：\", accuracy_NB)\n",
    "print(classification_report(y_test, y_test_NB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca631b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
      "[1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
      "Accuracy： 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.46      0.95      0.62        40\n",
      "         1.0       0.80      0.15      0.26        52\n",
      "\n",
      "    accuracy                           0.50        92\n",
      "   macro avg       0.63      0.55      0.44        92\n",
      "weighted avg       0.65      0.50      0.42        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=60)  \n",
    "\n",
    "knn_classifier.fit(x_train, y_train)\n",
    "\n",
    "y_test_KNN = knn_classifier.predict(x_test)\n",
    "print(y_test_KNN)\n",
    "print(y_test)\n",
    "\n",
    "accuracy_KNN = accuracy_score(y_test, y_test_KNN)\n",
    "print(\"Accuracy：\", accuracy_KNN)\n",
    "print(classification_report(y_test, y_test_KNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb41488e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0.]\n",
      "[1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
      "Accuracy: 0.6847826086956522\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.60      0.62        40\n",
      "         1.0       0.71      0.75      0.73        52\n",
      "\n",
      "    accuracy                           0.68        92\n",
      "   macro avg       0.68      0.68      0.68        92\n",
      "weighted avg       0.68      0.68      0.68        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "modelGB = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42)\n",
    "\n",
    "modelGB.fit(x_train, y_train)\n",
    "\n",
    "y_test_GB = modelGB.predict(x_test)\n",
    "print(y_test_GB)\n",
    "print(y_test)\n",
    "\n",
    "accuracy_GB = accuracy_score(y_test, y_test_GB)\n",
    "print(\"Accuracy:\", accuracy_GB)\n",
    "\n",
    "print(classification_report(y_test, y_test_GB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "389aed5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
      "[1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
      "Accuracy: 0.6847826086956522\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.57      0.61        40\n",
      "         1.0       0.70      0.77      0.73        52\n",
      "\n",
      "    accuracy                           0.68        92\n",
      "   macro avg       0.68      0.67      0.67        92\n",
      "weighted avg       0.68      0.68      0.68        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "modelRF = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "modelRF.fit(x_train, y_train)\n",
    "\n",
    "y_test_RF = modelRF.predict(x_test)\n",
    "print(y_test_RF)\n",
    "print(y_test)\n",
    "\n",
    "accuracy_RF = accuracy_score(y_test, y_test_RF)\n",
    "print(\"Accuracy:\", accuracy_RF)\n",
    "\n",
    "print(classification_report(y_test, y_test_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c3fb9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
      "[1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
      "Accuracy: 0.6195652173913043\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.60      0.58        40\n",
      "         1.0       0.67      0.63      0.65        52\n",
      "\n",
      "    accuracy                           0.62        92\n",
      "   macro avg       0.62      0.62      0.62        92\n",
      "weighted avg       0.62      0.62      0.62        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "modelSVC = SVC(kernel='linear')\n",
    "\n",
    "modelSVC.fit(x_train, y_train)\n",
    "\n",
    "y_test_SVC = modelSVC.predict(x_test)\n",
    "print(y_test_SVC)\n",
    "print(y_test)\n",
    "\n",
    "accuracy_SVC = accuracy_score(y_test, y_test_SVC)\n",
    "print(\"Accuracy:\", accuracy_SVC)\n",
    "\n",
    "print(classification_report(y_test, y_test_SVC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29426582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0.\n",
      " 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
      "[1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
      "Accuracy: 0.6956521739130435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.62      0.64        40\n",
      "         1.0       0.72      0.75      0.74        52\n",
      "\n",
      "    accuracy                           0.70        92\n",
      "   macro avg       0.69      0.69      0.69        92\n",
      "weighted avg       0.69      0.70      0.69        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "modelLR = LogisticRegression()\n",
    "\n",
    "modelLR.fit(x_train, y_train)\n",
    "\n",
    "y_test_LR = modelLR.predict(x_test)\n",
    "print(y_test_LR)\n",
    "print(y_test)\n",
    "\n",
    "accuracy_LR = accuracy_score(y_test, y_test_LR)\n",
    "print(\"Accuracy:\", accuracy_LR)\n",
    "\n",
    "print(classification_report(y_test, y_test_LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55415ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:54:24] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 0.6413043478260869\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.53      0.56        40\n",
      "         1.0       0.67      0.73      0.70        52\n",
      "\n",
      "    accuracy                           0.64        92\n",
      "   macro avg       0.63      0.63      0.63        92\n",
      "weighted avg       0.64      0.64      0.64        92\n",
      "\n",
      "[1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#XGB\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, random_state=42)\n",
    "\n",
    "xgb_clf.fit(x_train, y_train)\n",
    "\n",
    "y_test_XGB = xgb_clf.predict(x_test)\n",
    "\n",
    "accuracy_XGB = accuracy_score(y_test, y_test_XGB)\n",
    "print(\"Accuracy:\", accuracy_XGB)\n",
    "\n",
    "print(classification_report(y_test, y_test_XGB))\n",
    "print(y_test_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43d271c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6413043478260869\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.57      0.58        40\n",
      "         1.0       0.68      0.69      0.69        52\n",
      "\n",
      "    accuracy                           0.64        92\n",
      "   macro avg       0.63      0.63      0.63        92\n",
      "weighted avg       0.64      0.64      0.64        92\n",
      "\n",
      "[1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "base_model = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "\n",
    "adaboost_clf = AdaBoostClassifier(base_estimator=base_model, n_estimators=100, random_state=42)\n",
    "\n",
    "adaboost_clf.fit(x_train, y_train)\n",
    "\n",
    "y_test_ABC = adaboost_clf.predict(x_test)\n",
    "\n",
    "accuracy_ABC = accuracy_score(y_test, y_test_ABC)\n",
    "print(\"Accuracy:\", accuracy_ABC)\n",
    "\n",
    "print(classification_report(y_test, y_test_ABC))\n",
    "print(y_test_ABC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15c03c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:54:26] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 0.717391304347826\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.57      0.64        40\n",
      "         1.0       0.72      0.83      0.77        52\n",
      "\n",
      "    accuracy                           0.72        92\n",
      "   macro avg       0.72      0.70      0.70        92\n",
      "weighted avg       0.72      0.72      0.71        92\n",
      "\n",
      "[0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "clf1 = LogisticRegression(random_state=42)\n",
    "clf2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf3 = SVC(random_state=42, probability=True)\n",
    "clf4 = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, random_state=42)\n",
    "\n",
    "voting_clf1 = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('svc', clf3), ('xgb', clf4)], voting='soft')\n",
    " \n",
    "voting_clf1.fit(x_train, y_train)\n",
    "\n",
    "y_test_VC1 = voting_clf1.predict(x_test)\n",
    "\n",
    "accuracy_VC1 = accuracy_score(y_test, y_test_VC1)\n",
    "print(\"Accuracy:\", accuracy_VC1)\n",
    "\n",
    "print(classification_report(y_test, y_test_VC1))\n",
    "print(y_test_VC1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d67bbb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
      "[1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
      "Accuracy: 0.7065217391304348\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.60      0.64        40\n",
      "         1.0       0.72      0.79      0.75        52\n",
      "\n",
      "    accuracy                           0.71        92\n",
      "   macro avg       0.70      0.69      0.70        92\n",
      "weighted avg       0.70      0.71      0.70        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf2 = SVC(kernel='linear', C=1.0, random_state=42, probability=True)\n",
    "\n",
    "voting_clf2 = VotingClassifier(estimators=[('rf', clf1), ('svm', clf2)], voting='soft')\n",
    "\n",
    "voting_clf2.fit(x_train, y_train)\n",
    "\n",
    "y_test_VC2 = voting_clf2.predict(x_test)\n",
    "print(y_test_VC2)\n",
    "print(y_test)\n",
    "\n",
    "accuracy_VC2 = accuracy_score(y_test, y_test_VC2)\n",
    "print(\"Accuracy:\", accuracy_VC2)\n",
    "\n",
    "print(classification_report(y_test, y_test_VC2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "688af0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6956521739130435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.55      0.61        40\n",
      "         1.0       0.70      0.81      0.75        52\n",
      "\n",
      "    accuracy                           0.70        92\n",
      "   macro avg       0.69      0.68      0.68        92\n",
      "weighted avg       0.69      0.70      0.69        92\n",
      "\n",
      "[0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0.\n",
      " 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "clf1 = LogisticRegression(random_state=42)\n",
    "clf2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf3 = SVC(random_state=42, probability=True)\n",
    "\n",
    "voting_clf3 = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('svc', clf3)], voting='soft')\n",
    "\n",
    "voting_clf3.fit(x_train, y_train)\n",
    "\n",
    "y_test_VC3 = voting_clf3.predict(x_test)\n",
    "\n",
    "accuracy_VC3 = accuracy_score(y_test, y_test_VC3)\n",
    "print(\"Accuracy:\", accuracy_VC3)\n",
    "\n",
    "print(classification_report(y_test, y_test_VC3))\n",
    "print(y_test_VC3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66195af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6847826086956522\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.53      0.59        40\n",
      "         1.0       0.69      0.81      0.74        52\n",
      "\n",
      "    accuracy                           0.68        92\n",
      "   macro avg       0.68      0.67      0.67        92\n",
      "weighted avg       0.68      0.68      0.68        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier \n",
    "\n",
    "stack1 = LogisticRegression(random_state=42)\n",
    "stack2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "stack3 = SVC(random_state=42, probability=True)\n",
    "stack = StackingClassifier(estimators=[( 'lr' , stack1), ( 'RF' , stack2), ( 'SVC' , stack3)])\n",
    "\n",
    "stack.fit(x_train, y_train) \n",
    " \n",
    "y_test_stack = stack.predict(x_test) \n",
    " \n",
    "accuracy_stack = accuracy_score(y_test, y_test_stack)\n",
    "print(\"Accuracy:\", accuracy_stack)\n",
    "\n",
    "print(classification_report(y_test, y_test_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84dc2f15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating GaussianNB...\n",
      "Average accuracy: 0.69\n",
      "\n",
      "\n",
      "Evaluating KNeighborsClassifier...\n",
      "Average accuracy: 0.51\n",
      "\n",
      "\n",
      "Evaluating GradientBoostingClassifier...\n",
      "Average accuracy: 0.68\n",
      "\n",
      "\n",
      "Evaluating RandomForestClassifier...\n",
      "Average accuracy: 0.68\n",
      "\n",
      "\n",
      "Evaluating SVC...\n",
      "Average accuracy: 0.68\n",
      "\n",
      "\n",
      "Evaluating LogisticRegression...\n",
      "Average accuracy: 0.68\n",
      "\n",
      "\n",
      "Evaluating XGBClassifier...\n",
      "[19:54:33] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:54:33] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:54:33] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:54:34] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:54:34] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Average accuracy: 0.64\n",
      "\n",
      "\n",
      "Evaluating AdaBoostClassifier...\n",
      "Average accuracy: 0.67\n",
      "\n",
      "\n",
      "Evaluating VotingClassifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:54:35] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:54:35] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:54:36] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:54:36] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\cyclopeptide\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:54:36] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Average accuracy: 0.68\n",
      "\n",
      "\n",
      "Evaluating VotingClassifier...\n",
      "Average accuracy: 0.69\n",
      "\n",
      "\n",
      "Evaluating VotingClassifier...\n",
      "Average accuracy: 0.70\n",
      "\n",
      "\n",
      "Evaluating StackingClassifier...\n",
      "Average accuracy: 0.57\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "models = [nb_classifier, knn_classifier, modelGB, modelRF, modelSVC, modelLR, xgb_clf, adaboost_clf, voting_clf1, voting_clf2, voting_clf3,stack]  # 假设这里有三个模型\n",
    "\n",
    "# 定义 k 折交叉验证\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for model in models:\n",
    "    print(f\"Evaluating {model.__class__.__name__}...\")\n",
    "\n",
    "    accuracies = []\n",
    "    for train_index, test_index in kfold.split(X_scaled, y_train1):\n",
    "        x_train, x_test = X_filtered[train_index], X_filtered[test_index]\n",
    "        y_train, y_test = y_train1[train_index], y_train1[test_index]\n",
    "\n",
    "        \n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        \n",
    "        y_pred = model.predict(x_test)\n",
    "\n",
    "       \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "  \n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    print(f\"Average accuracy: {mean_accuracy:.2f}\")\n",
    "\n",
    "  \n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee469f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.601427815970386 [20]\n",
      "0.6308831306187204 [20, 93]\n",
      "0.66351136964569 [20, 93, 28]\n",
      "0.6863564251718668 [20, 93, 28, 26]\n",
      "0.706081438392385 [20, 93, 28, 26, 0]\n",
      "0.725753569539926 [20, 93, 28, 26, 0, 325]\n",
      "0.7355896351136965 [20, 93, 28, 26, 0, 325, 21]\n",
      "0.7518773135906927 [20, 93, 28, 26, 0, 325, 21, 161]\n",
      "0.755156002115283 [20, 93, 28, 26, 0, 325, 21, 161, 298]\n",
      "0.7615547329455314 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36]\n",
      "0.7713379164463248 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367]\n",
      "0.7777895293495506 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1]\n",
      "0.7844526705446854 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149]\n",
      "0.8007403490216817 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318]\n",
      "0.8040719196192491 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82]\n",
      "0.8106292966684293 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100]\n",
      "0.8139079851930197 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62]\n",
      "0.8204653622421999 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6]\n",
      "0.8236911686938129 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12]\n",
      "0.8236911686938129 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35]\n",
      "0.826969857218403 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208]\n",
      "0.8301956636700158 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172]\n",
      "0.8301956636700158 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44]\n",
      "0.8302485457429931 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71]\n",
      "0.8334743521946061 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101]\n",
      "0.8334743521946061 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188]\n",
      "0.8335272342675832 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225]\n",
      "0.8367001586462189 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321]\n",
      "0.8334743521946061 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374]\n",
      "0.830195663670016 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405]\n",
      "0.826969857218403 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104]\n",
      "0.826969857218403 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123]\n",
      "0.8301956636700158 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198]\n",
      "0.8269169751454257 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113]\n",
      "0.8334214701216288 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68]\n",
      "0.8367530407191962 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222]\n",
      "0.8335272342675832 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116]\n",
      "0.8334743521946061 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59]\n",
      "0.8334743521946061 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119]\n",
      "0.830195663670016 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143]\n",
      "0.8302485457429931 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7]\n",
      "0.8334743521946061 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168]\n",
      "0.8334743521946061 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175]\n",
      "0.8367001586462189 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55]\n",
      "0.830195663670016 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194]\n",
      "0.8367001586462189 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194, 42]\n",
      "0.8334214701216288 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194, 42, 3]\n",
      "0.8367001586462189 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194, 42, 3, 380]\n",
      "0.8367001586462189 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194, 42, 3, 380, 138]\n",
      "0.8367001586462189 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194, 42, 3, 380, 138, 351]\n",
      "0.8367001586462189 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194, 42, 3, 380, 138, 351, 65]\n",
      "0.8399788471708091 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194, 42, 3, 380, 138, 351, 65, 344]\n",
      "0.8367001586462189 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194, 42, 3, 380, 138, 351, 65, 344, 8]\n",
      "0.8399788471708091 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194, 42, 3, 380, 138, 351, 65, 344, 8, 52]\n",
      "0.8367001586462189 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194, 42, 3, 380, 138, 351, 65, 344, 8, 52, 58]\n",
      "0.8334743521946061 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194, 42, 3, 380, 138, 351, 65, 344, 8, 52, 58, 197]\n",
      "0.8367001586462189 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194, 42, 3, 380, 138, 351, 65, 344, 8, 52, 58, 197, 18]\n",
      "0.8399259650978319 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194, 42, 3, 380, 138, 351, 65, 344, 8, 52, 58, 197, 18, 56]\n",
      "0.8334214701216288 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194, 42, 3, 380, 138, 351, 65, 344, 8, 52, 58, 197, 18, 56, 137]\n",
      "0.8334214701216288 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194, 42, 3, 380, 138, 351, 65, 344, 8, 52, 58, 197, 18, 56, 137, 364]\n",
      "0.8301956636700158 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194, 42, 3, 380, 138, 351, 65, 344, 8, 52, 58, 197, 18, 56, 137, 364, 268]\n",
      "0.8268640930724483 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194, 42, 3, 380, 138, 351, 65, 344, 8, 52, 58, 197, 18, 56, 137, 364, 268, 237]\n",
      "0.8367001586462189 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194, 42, 3, 380, 138, 351, 65, 344, 8, 52, 58, 197, 18, 56, 137, 364, 268, 237, 47]\n",
      "0.8334743521946061 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194, 42, 3, 380, 138, 351, 65, 344, 8, 52, 58, 197, 18, 56, 137, 364, 268, 237, 47, 373]\n",
      "0.8367001586462189 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194, 42, 3, 380, 138, 351, 65, 344, 8, 52, 58, 197, 18, 56, 137, 364, 268, 237, 47, 373, 356]\n",
      "0.8302485457429931 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194, 42, 3, 380, 138, 351, 65, 344, 8, 52, 58, 197, 18, 56, 137, 364, 268, 237, 47, 373, 356, 146]\n",
      "0.8334743521946061 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194, 42, 3, 380, 138, 351, 65, 344, 8, 52, 58, 197, 18, 56, 137, 364, 268, 237, 47, 373, 356, 146, 64]\n",
      "0.8334743521946061 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194, 42, 3, 380, 138, 351, 65, 344, 8, 52, 58, 197, 18, 56, 137, 364, 268, 237, 47, 373, 356, 146, 64, 91]\n",
      "0.8335272342675832 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194, 42, 3, 380, 138, 351, 65, 344, 8, 52, 58, 197, 18, 56, 137, 364, 268, 237, 47, 373, 356, 146, 64, 91, 199]\n",
      "0.8301956636700158 [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194, 42, 3, 380, 138, 351, 65, 344, 8, 52, 58, 197, 18, 56, 137, 364, 268, 237, 47, 373, 356, 146, 64, 91, 199, 2]\n"
     ]
    }
   ],
   "source": [
    "def feature_selection(X, y):\n",
    "    remain_list = []  # List of selected feature indices\n",
    "    all_list = list(range(len(X[0])))  # Indices of all features\n",
    "    max_iterations = 70  # Set maximum iterations\n",
    "    iteration_count = 0 \n",
    "\n",
    "    for idx in range(70):  # Stop after 70 iterations\n",
    "        all_r2 = []\n",
    "        for id, each in enumerate(all_list):\n",
    "            print(f\"{id}/{len(all_list)}\", end='\\r')  # Display current progress\n",
    "\n",
    "            if each in remain_list:  # Skip if feature is already selected\n",
    "                all_r2.append(-10)\n",
    "                continue\n",
    "\n",
    "            temp_remain_list = remain_list + [each]  # Add the current feature to the candidate list\n",
    "            X_new = X[:, temp_remain_list]  # Use the selected feature subset\n",
    "            kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            model = voting_clf3\n",
    "            acc = cross_val_score(model, X_new, y, cv=kfold, n_jobs=-1).mean()  # Calculate accuracy\n",
    "            all_r2.append(acc.mean())  # Add current feature's accuracy to the list\n",
    "\n",
    "        max_id = np.argmax(all_r2)  # Index of the feature with the maximum accuracy\n",
    "        remain_list.append(all_list[max_id])  # Add that feature to the selected list\n",
    "\n",
    "        print(np.max(all_r2), remain_list)  # Print the maximum accuracy and selected features\n",
    "\n",
    "        iteration_count += 1 \n",
    "\n",
    "    return all_r2, remain_list\n",
    "\n",
    "all_r2, y_in_removed_lists = feature_selection(X_filtered, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9347b467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[243  39  84   1 134 217 300 273 276 185  73 262 122   5  46 174  36 158\n",
      "  52 152 210 133 101 108   7  83 291 129 274 189 303  82 279  59  29  34\n",
      " 164 252  43 254 192 136  51 200 183 227  20  15 261  23  25 105 205 302\n",
      " 176 194 266 203  21 209 301 223 201 110  30 155 159 116 123 222 127 187\n",
      "   8  67 139  41 290  79  85  44  93  71 268 154  10  90 204 295  69 141\n",
      "  50 214 229 118 168 135  48 115 153  72 126 172  57 149 144   9  98  13\n",
      " 207 211 246  74 213 267 245 240 146 182  64 226  53 173 143 293  33  75\n",
      "   6 286 230 269  99  11  63  91 255  22 191  68  76  96 177  94 128  95\n",
      " 239 150 138 218 145 250 297 117  16 280 188 119 169 249 131 281  24 219\n",
      "  47 196 206 113 231 179  28  80 142 260 271 170 285 277 140  65  62 130\n",
      "  60 107 292  78   3 208 258  17 157 106 298 299 181  42 114  32 263  27\n",
      "  49 264 175 242 287 237  45 221 294 278 289 100  26  35 224 120]\n",
      "[ 12  66 256 184 235   0 163 272 166 186 199 193 156 296  31 284  87 241\n",
      " 167 102   4 215 178 234 305 282 304 202 165 228 162  97 232 171  77  58\n",
      " 220 132 251 283 197 125 104 160 244 236 257  18 112  19 238 225  81  55\n",
      " 248 161   2 198 212 103  92 111 265 147 247 151  40 275  14 233 180 216\n",
      " 137  61 109  38 121 124 148 259  56 253  37 190 270  86  54 195  89  70\n",
      " 288  88]\n",
      "122\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "X_new = X_filtered[:, [20, 93, 28, 26, 0, 325, 21, 161, 298, 36, 367, 1, 149, 318, 82, 100, 62, 6, 12, 35, 208, 172, 44, 71, 101, 188, 225, 321, 374, 405, 104, 123, 198, 113, 68, 222, 116, 59, 119, 143, 7, 168, 175, 55, 194, 42, 3, 380, 138, 351, 65, 344, 8, 52, 58, 197, 18, 56]]\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "for train_index, test_index in sss.split(X_new, y_train1):\n",
    "    x_train, x_test = X_new[train_index], X_new[test_index]\n",
    "    y_train, y_test = y_train1[train_index], y_train1[test_index]\n",
    "print(train_index)\n",
    "print(test_index)\n",
    "train1=0\n",
    "train0=0\n",
    "for i in y_train:\n",
    "    if i==1:\n",
    "        train1+=1\n",
    "    else:\n",
    "        train0+=1\n",
    "print(train1)\n",
    "print(train0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75372e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1.\n",
      " 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
      "[1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
      "Accuracy: 0.6956521739130435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.57      0.62        40\n",
      "         1.0       0.71      0.79      0.75        52\n",
      "\n",
      "    accuracy                           0.70        92\n",
      "   macro avg       0.69      0.68      0.68        92\n",
      "weighted avg       0.69      0.70      0.69        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "modelRF = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "modelRF.fit(x_train, y_train)\n",
    "\n",
    "y_test_RF = modelRF.predict(x_test)\n",
    "print(y_test_RF)\n",
    "print(y_test)\n",
    "\n",
    "accuracy_RF = accuracy_score(y_test, y_test_RF)\n",
    "print(\"Accuracy:\", accuracy_RF)\n",
    "\n",
    "print(classification_report(y_test, y_test_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6392832e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0.\n",
      " 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1.]\n",
      "[1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
      "Accuracy: 0.6195652173913043\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.60      0.58        40\n",
      "         1.0       0.67      0.63      0.65        52\n",
      "\n",
      "    accuracy                           0.62        92\n",
      "   macro avg       0.62      0.62      0.62        92\n",
      "weighted avg       0.62      0.62      0.62        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "modelSVC = SVC(kernel='linear')\n",
    "\n",
    "modelSVC.fit(x_train, y_train)\n",
    "\n",
    "y_test_SVC = modelSVC.predict(x_test)\n",
    "print(y_test_SVC)\n",
    "print(y_test)\n",
    "\n",
    "accuracy_SVC = accuracy_score(y_test, y_test_SVC)\n",
    "print(\"Accuracy:\", accuracy_SVC)\n",
    "\n",
    "print(classification_report(y_test, y_test_SVC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "291df50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0.\n",
      " 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1.]\n",
      "[1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
      "Accuracy: 0.6521739130434783\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.53      0.57        40\n",
      "         1.0       0.67      0.75      0.71        52\n",
      "\n",
      "    accuracy                           0.65        92\n",
      "   macro avg       0.65      0.64      0.64        92\n",
      "weighted avg       0.65      0.65      0.65        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "modelLR = LogisticRegression()\n",
    "\n",
    "modelLR.fit(x_train, y_train)\n",
    "\n",
    "y_test_LR = modelLR.predict(x_test)\n",
    "print(y_test_LR)\n",
    "print(y_test)\n",
    "\n",
    "accuracy_LR = accuracy_score(y_test, y_test_LR)\n",
    "print(\"Accuracy:\", accuracy_LR)\n",
    "\n",
    "print(classification_report(y_test, y_test_LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "484e7051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7391304347826086\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.65      0.68        40\n",
      "         1.0       0.75      0.81      0.78        52\n",
      "\n",
      "    accuracy                           0.74        92\n",
      "   macro avg       0.74      0.73      0.73        92\n",
      "weighted avg       0.74      0.74      0.74        92\n",
      "\n",
      "[0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "clf1 = LogisticRegression(random_state=42)\n",
    "clf2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf3 = SVC(random_state=42, probability=True)\n",
    "\n",
    "voting_clf3 = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('svc', clf3)], voting='soft')\n",
    "\n",
    "voting_clf3.fit(x_train, y_train)\n",
    "\n",
    "y_test_VC3 = voting_clf3.predict(x_test)\n",
    "\n",
    "accuracy_VC3 = accuracy_score(y_test, y_test_VC3)\n",
    "print(\"Accuracy:\", accuracy_VC3)\n",
    "\n",
    "print(classification_report(y_test, y_test_VC3))\n",
    "print(y_test_VC3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4e72e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_307.pkl']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(voting_clf3, 'model_307.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496b835f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cyclopeptide]",
   "language": "python",
   "name": "conda-env-cyclopeptide-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
